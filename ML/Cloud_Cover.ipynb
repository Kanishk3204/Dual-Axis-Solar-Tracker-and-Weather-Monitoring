{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QxbUmwBTMbp"
      },
      "outputs": [],
      "source": [
        "!pip install openmeteo-requests\n",
        "!pip install requests-cache retry-requests numpy pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openmeteo_requests\n",
        "\n",
        "import requests_cache\n",
        "import pandas as pd\n",
        "from retry_requests import retry\n",
        "\n",
        "# Setup the Open-Meteo API client with cache and retry on error\n",
        "cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
        "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
        "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
        "\n",
        "# Make sure all required weather variables are listed here\n",
        "# The order of variables in hourly or daily is important to assign them correctly below\n",
        "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "params = {\n",
        "\t\"latitude\": 22.8101,\n",
        "\t\"longitude\": 86.2634,\n",
        "\t\"start_date\": \"2024-02-15\",\n",
        "\t\"end_date\": \"2024-03-07\",\n",
        "\t\"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \"precipitation\", \"surface_pressure\", \"cloud_cover\", \"wind_speed_100m\", \"wind_direction_100m\", \"is_day\", \"sunshine_duration\", \"direct_radiation\", \"diffuse_radiation\"],\n",
        "\t\"timezone\": \"auto\"\n",
        "}\n",
        "responses = openmeteo.weather_api(url, params=params)\n",
        "\n",
        "# Process first location. Add a for-loop for multiple locations or weather models\n",
        "response = responses[0]\n",
        "print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
        "print(f\"Elevation {response.Elevation()} m asl\")\n",
        "print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
        "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
        "\n",
        "# Process hourly data. The order of variables needs to be the same as requested.\n",
        "hourly = response.Hourly()\n",
        "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
        "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
        "hourly_dew_point_2m = hourly.Variables(2).ValuesAsNumpy()\n",
        "hourly_precipitation = hourly.Variables(3).ValuesAsNumpy()\n",
        "hourly_surface_pressure = hourly.Variables(4).ValuesAsNumpy()\n",
        "hourly_cloud_cover = hourly.Variables(5).ValuesAsNumpy()\n",
        "hourly_wind_speed_100m = hourly.Variables(6).ValuesAsNumpy()\n",
        "hourly_wind_direction_100m = hourly.Variables(7).ValuesAsNumpy()\n",
        "hourly_is_day = hourly.Variables(8).ValuesAsNumpy()\n",
        "hourly_sunshine_duration = hourly.Variables(9).ValuesAsNumpy()\n",
        "hourly_direct_radiation = hourly.Variables(10).ValuesAsNumpy()\n",
        "hourly_diffuse_radiation = hourly.Variables(11).ValuesAsNumpy()\n",
        "\n",
        "hourly_data = {\"date\": pd.date_range(\n",
        "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
        "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
        "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
        "\tinclusive = \"left\"\n",
        ")}\n",
        "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
        "hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
        "hourly_data[\"dew_point_2m\"] = hourly_dew_point_2m\n",
        "hourly_data[\"precipitation\"] = hourly_precipitation\n",
        "hourly_data[\"surface_pressure\"] = hourly_surface_pressure\n",
        "hourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
        "hourly_data[\"wind_speed_100m\"] = hourly_wind_speed_100m\n",
        "hourly_data[\"wind_direction_100m\"] = hourly_wind_direction_100m\n",
        "hourly_data[\"is_day\"] = hourly_is_day\n",
        "hourly_data[\"sunshine_duration\"] = hourly_sunshine_duration\n",
        "hourly_data[\"direct_radiation\"] = hourly_direct_radiation\n",
        "hourly_data[\"diffuse_radiation\"] = hourly_diffuse_radiation\n",
        "\n",
        "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
        "print(hourly_dataframe)\n",
        "# import openmeteo_requests\n",
        "\n",
        "# import requests_cache\n",
        "# import pandas as pd\n",
        "# from retry_requests import retry\n",
        "\n",
        "# # Setup the Open-Meteo API client with cache and retry on error\n",
        "# cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
        "# retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
        "# openmeteo = openmeteo_requests.Client(session = retry_session)\n",
        "\n",
        "# # Make sure all required weather variables are listed here\n",
        "# # The order of variables in hourly or daily is important to assign them correctly below\n",
        "# url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "# params = {\n",
        "# \t\"latitude\": 22.8101,\n",
        "# \t\"longitude\": 86.2634,\n",
        "# \t\"start_date\": \"2024-02-15\",\n",
        "# \t\"end_date\": \"2024-03-07\",\n",
        "# \t\"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \"precipitation\", \"surface_pressure\", \"cloud_cover\", \"wind_speed_100m\", \"wind_direction_100m\", \"is_day\", \"sunshine_duration\", \"direct_radiation\", \"diffuse_radiation\"],\n",
        "# \t\"timezone\": \"auto\"\n",
        "# }\n",
        "# responses = openmeteo.weather_api(url, params=params)\n",
        "\n",
        "# # Process first location. Add a for-loop for multiple locations or weather models\n",
        "# response = responses[0]\n",
        "# print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
        "# print(f\"Elevation {response.Elevation()} m asl\")\n",
        "# print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
        "# print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
        "\n",
        "# # Process hourly data. The order of variables needs to be the same as requested.\n",
        "# hourly = response.Hourly()\n",
        "# hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
        "# hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
        "# hourly_dew_point_2m = hourly.Variables(2).ValuesAsNumpy()\n",
        "# hourly_precipitation = hourly.Variables(3).ValuesAsNumpy()\n",
        "# hourly_surface_pressure = hourly.Variables(4).ValuesAsNumpy()\n",
        "# hourly_cloud_cover = hourly.Variables(5).ValuesAsNumpy()\n",
        "# hourly_wind_speed_100m = hourly.Variables(6).ValuesAsNumpy()\n",
        "# hourly_wind_direction_100m = hourly.Variables(7).ValuesAsNumpy()\n",
        "# hourly_is_day = hourly.Variables(8).ValuesAsNumpy()\n",
        "# hourly_sunshine_duration = hourly.Variables(9).ValuesAsNumpy()\n",
        "# hourly_direct_radiation = hourly.Variables(10).ValuesAsNumpy()\n",
        "# hourly_diffuse_radiation = hourly.Variables(11).ValuesAsNumpy()\n",
        "\n",
        "# hourly_data = {\"date\": pd.date_range(\n",
        "# \tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
        "# \tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
        "# \tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
        "# \tinclusive = \"left\"\n",
        "# )}\n",
        "# hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
        "# hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
        "# hourly_data[\"dew_point_2m\"] = hourly_dew_point_2m\n",
        "# hourly_data[\"precipitation\"] = hourly_precipitation\n",
        "# hourly_data[\"surface_pressure\"] = hourly_surface_pressure\n",
        "# hourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
        "# hourly_data[\"wind_speed_100m\"] = hourly_wind_speed_100m\n",
        "# hourly_data[\"wind_direction_100m\"] = hourly_wind_direction_100m\n",
        "# hourly_data[\"is_day\"] = hourly_is_day\n",
        "# hourly_data[\"sunshine_duration\"] = hourly_sunshine_duration\n",
        "# hourly_data[\"direct_radiation\"] = hourly_direct_radiation\n",
        "# hourly_data[\"diffuse_radiation\"] = hourly_diffuse_radiation\n",
        "\n",
        "# hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
        "# print(hourly_dataframe)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY8w7V6ATZVQ",
        "outputId": "959df745-bd4e-48b7-f929-62c1614a3f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coordinates 22.811948776245117°N 86.25°E\n",
            "Elevation 135.0 m asl\n",
            "Timezone b'Asia/Kolkata' b'IST'\n",
            "Timezone difference to GMT+0 19800 s\n",
            "                         date  temperature_2m  relative_humidity_2m  \\\n",
            "0   2024-02-14 18:30:00+00:00       18.407499             92.138802   \n",
            "1   2024-02-14 19:30:00+00:00       19.757500             85.508797   \n",
            "2   2024-02-14 20:30:00+00:00       18.057501             91.826599   \n",
            "3   2024-02-14 21:30:00+00:00       19.057501             88.470024   \n",
            "4   2024-02-14 22:30:00+00:00       16.707500             95.947205   \n",
            "..                        ...             ...                   ...   \n",
            "523 2024-03-07 13:30:00+00:00       23.607500             77.812508   \n",
            "524 2024-03-07 14:30:00+00:00       22.957500             78.202972   \n",
            "525 2024-03-07 15:30:00+00:00       21.457500             84.882576   \n",
            "526 2024-03-07 16:30:00+00:00       20.957500             84.829567   \n",
            "527 2024-03-07 17:30:00+00:00       21.657499             81.012062   \n",
            "\n",
            "     dew_point_2m  precipitation  surface_pressure  cloud_cover  \\\n",
            "0       17.107500            0.0       1000.767761    27.600002   \n",
            "1       17.257500            0.0       1000.348267    21.000000   \n",
            "2       16.707500            0.0        999.764648    18.600002   \n",
            "3       17.107500            0.0        999.621704    16.500000   \n",
            "4       16.057499            0.0        999.887939    12.900001   \n",
            "..            ...            ...               ...          ...   \n",
            "523     19.507500            0.0        996.318420    14.400001   \n",
            "524     18.957500            0.0        997.072021     0.600000   \n",
            "525     18.807501            0.0        997.584045     0.600000   \n",
            "526     18.307501            0.0        997.951172    38.400002   \n",
            "527     18.257500            0.0        997.890076    22.799999   \n",
            "\n",
            "     wind_speed_100m  wind_direction_100m  is_day  sunshine_duration  \\\n",
            "0          13.202726           101.003494     0.0                0.0   \n",
            "1           5.904439           127.568665     0.0                0.0   \n",
            "2           0.720000            90.000000     0.0                0.0   \n",
            "3           3.075841           200.556122     0.0                0.0   \n",
            "4           6.489992           266.820221     0.0                0.0   \n",
            "..               ...                  ...     ...                ...   \n",
            "523        16.215992            92.544754     0.0                0.0   \n",
            "524        16.119801           113.702660     0.0                0.0   \n",
            "525        12.181624           124.159744     0.0                0.0   \n",
            "526         8.287822           124.380402     0.0                0.0   \n",
            "527        13.979871            11.888645     0.0                0.0   \n",
            "\n",
            "     direct_radiation  diffuse_radiation  \n",
            "0                 0.0                0.0  \n",
            "1                 0.0                0.0  \n",
            "2                 0.0                0.0  \n",
            "3                 0.0                0.0  \n",
            "4                 0.0                0.0  \n",
            "..                ...                ...  \n",
            "523               0.0                0.0  \n",
            "524               0.0                0.0  \n",
            "525               0.0                0.0  \n",
            "526               0.0                0.0  \n",
            "527               0.0                0.0  \n",
            "\n",
            "[528 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Change data types accordingly\n",
        "hourly_dataframe['date'] = pd.to_datetime(hourly_dataframe['date'])\n",
        "# # hourly_dataframe['date_only'] = hourly_dataframe['date'].dt.date\n",
        "# # hourly_dataframe['time_only'] = hourly_dataframe['date'].dt.time\n",
        "\n",
        "# # Displaying the DataFrame with the newly added columns\n",
        "# print(hourly_dataframe) # Convert 'date' column to datetime format\n",
        "\n",
        "# # Convert numerical attributes to float or int\n",
        "# numerical_attributes = ['temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'precipitation',\n",
        "#                         'surface_pressure', 'wind_speed_100m', 'wind_direction_100m', 'sunshine_duration']\n",
        "# hourly_dataframe[numerical_attributes] = hourly_dataframe[numerical_attributes].astype(float)  # Convert numerical attributes to float\n",
        "\n",
        "# # Convert boolean attribute to bool\n",
        "# hourly_dataframe['is_day'] = hourly_dataframe['is_day'].astype(bool)\n",
        "\n",
        "# # Ensure that 'cloud_cover' attribute remains unchanged, as it is likely the target variable\n",
        "\n",
        "# # Print the data types of attributes after conversion\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming hourly_data is a pandas DataFrame\n",
        "hourly_data[\"temperature_2m\"] = pd.to_numeric(hourly_data[\"temperature_2m\"])\n",
        "hourly_data[\"relative_humidity_2m\"] = pd.to_numeric(hourly_data[\"relative_humidity_2m\"])\n",
        "hourly_data[\"dew_point_2m\"] = pd.to_numeric(hourly_data[\"dew_point_2m\"])\n",
        "hourly_data[\"precipitation\"] = pd.to_numeric(hourly_data[\"precipitation\"])\n",
        "hourly_data[\"surface_pressure\"] = pd.to_numeric(hourly_data[\"surface_pressure\"])\n",
        "hourly_data[\"cloud_cover\"] = pd.to_numeric(hourly_data[\"cloud_cover\"])\n",
        "hourly_data[\"wind_speed_100m\"] = pd.to_numeric(hourly_data[\"wind_speed_100m\"])\n",
        "hourly_data[\"wind_direction_100m\"] = pd.to_numeric(hourly_data[\"wind_direction_100m\"])\n",
        "hourly_data[\"is_day\"] = hourly_data[\"is_day\"].astype(bool)  # Assuming it's originally stored as a boolean\n",
        "hourly_data[\"sunshine_duration\"] = pd.to_numeric(hourly_data[\"sunshine_duration\"])\n",
        "hourly_data[\"direct_radiation\"] = pd.to_numeric(hourly_data[\"direct_radiation\"])\n",
        "hourly_data[\"diffuse_radiation\"] = pd.to_numeric(hourly_data[\"diffuse_radiation\"])\n",
        "print(hourly_dataframe.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GE1HEFOTqy-",
        "outputId": "ac02dc16-c211-4e6e-845c-c5465ee65d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date                    datetime64[ns, UTC]\n",
            "temperature_2m                      float32\n",
            "relative_humidity_2m                float32\n",
            "dew_point_2m                        float32\n",
            "precipitation                       float32\n",
            "surface_pressure                    float32\n",
            "cloud_cover                         float32\n",
            "wind_speed_100m                     float32\n",
            "wind_direction_100m                 float32\n",
            "is_day                              float32\n",
            "sunshine_duration                   float32\n",
            "direct_radiation                    float32\n",
            "diffuse_radiation                   float32\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without preprocessing"
      ],
      "metadata": {
        "id": "vVG6NnvNZP6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'hourly_dataframe' is your DataFrame with the necessary columns\n",
        "\n",
        "# Select features and target variable\n",
        "features = ['temperature_2m', 'relative_humidity_2m', 'dew_point_2m', 'precipitation',\n",
        "            'surface_pressure', 'wind_speed_100m', 'wind_direction_100m', 'sunshine_duration','direct_radiation','diffuse_radiation']\n",
        "target = 'cloud_cover'\n",
        "\n",
        "X = hourly_dataframe[features]\n",
        "y = hourly_dataframe[target]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
        "\n",
        "# Choose a model and train it\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "# Calculate MAE\n",
        "mae = np.mean(np.abs(y_test - predictions))\n",
        "# Calculate RSE\n",
        "rse = mse / np.var(y_test)\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"mae:\",mae)\n",
        "print(\"rse:\",rse)\n",
        "print(\"rmse:\",rmse)\n",
        "print(\"Coefficients:\")\n",
        "for feature, coef in zip(X.columns, model.coef_):\n",
        "    print(feature, ':', coef)\n",
        "print(\"Intercept:\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "id": "YyK6SvKAZPVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91eb1d81-96e7-4031-a48b-6cb88d39e02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 195.77444\n",
            "mae: 10.697839\n",
            "rse: 0.5241146\n",
            "rmse: 13.991942\n",
            "Coefficients:\n",
            "temperature_2m : 2.6245916\n",
            "relative_humidity_2m : 0.4823458\n",
            "dew_point_2m : -0.7531799\n",
            "precipitation : 19.641594\n",
            "surface_pressure : 0.12740988\n",
            "wind_speed_100m : 0.38813043\n",
            "wind_direction_100m : -0.019238045\n",
            "sunshine_duration : -0.0025914013\n",
            "direct_radiation : -0.047477826\n",
            "diffuse_radiation : 0.17429824\n",
            "Intercept:\n",
            "-193.97754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Initialize the Ridge Regression model\n",
        "ridge_model = Lasso(alpha=1)  # You can adjust the regularization strength (alpha) as needed\n",
        "\n",
        "# Train the model\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "print(\"Mean Squared Error (Ridge):\", mse_ridge)\n",
        "print(\"R-squared Score (Ridge):\", r2_ridge)\n",
        "print(\"Coefficients:\")\n",
        "for feature, coef in zip(X.columns, ridge_model.coef_):\n",
        "    print(feature, ':', coef)\n",
        "print(\"Intercept:\")\n",
        "print(ridge_model.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AkoaAIGUYe5",
        "outputId": "6066b6c0-da58-4026-aae0-a7aa885c97f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Ridge): 264.0563\n",
            "R-squared Score (Ridge): 0.2930856619544654\n",
            "Coefficients:\n",
            "temperature_2m : 1.8491389\n",
            "relative_humidity_2m : 0.32967922\n",
            "dew_point_2m : 0.0\n",
            "precipitation : 0.5215327\n",
            "surface_pressure : 0.019990621\n",
            "wind_speed_100m : 0.45151827\n",
            "wind_direction_100m : -0.012415821\n",
            "sunshine_duration : -0.0028961035\n",
            "direct_radiation : -0.045497846\n",
            "diffuse_radiation : 0.17885716\n",
            "Intercept:\n",
            "-71.90535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "LzR2s5KdWN6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "data=hourly_dataframe\n",
        "# Load the dataset\n",
        "#data = pd.read_csv('your_dataset.csv')  # Replace 'your_dataset.csv' with the actual filename\n",
        "data=data.drop(columns=['date'])\n",
        "# Calculate correlation between features and target variable\n",
        "correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Define the threshold for correlation\n",
        "correlation_threshold = 0.1  # Adjust as needed\n",
        "\n",
        "# Split features into two groups based on correlation\n",
        "high_corr_features = correlation[correlation >= correlation_threshold]\n",
        "high_corr_features=high_corr_features.drop('cloud_cover')\n",
        "high_corr_features = high_corr_features.index.tolist()\n",
        "low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "# # Prepare the data for imputation\n",
        "y = data['cloud_cover']\n",
        "X = data.drop(columns=['cloud_cover'])\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define preprocessing steps for high correlation features using IterativeImputer\n",
        "high_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer(max_iter=20, random_state=42)),  # IterativeImputer for high correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "# Define preprocessing steps for low correlation features using KNNImputer\n",
        "low_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors=5)),  # KNNImputer for low correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "# Define column transformer to apply different preprocessing steps to different feature groups\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('high_corr', high_corr_transformer, high_corr_features)\n",
        "        ,('low_corr', low_corr_transformer, low_corr_features)\n",
        "    ])\n",
        "\n",
        "preprocessor.fit(X_train,y_train)\n",
        "# Define the model\n",
        "model = RandomForestRegressor()\n",
        "\n",
        "# Create a pipeline with preprocessing and modeling steps\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', model)])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "#model.fit(X_train,y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = pipeline.predict(X_test)\n",
        "#predictions=model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "# Calculate MAE\n",
        "mae = np.mean(np.abs(y_test - predictions))\n",
        "# Calculate RSE\n",
        "rse = mse / np.var(y_test)\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"mae:\",mae)\n",
        "print(\"rse:\",rse)\n",
        "print(\"rmse:\",rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgyKv_TATyN4",
        "outputId": "7a45ba32-56e2-432c-ce4e-515792468edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 282.6816754557746\n",
            "mae: 11.932528605826622\n",
            "rse: 0.6583960277041276\n",
            "rmse: 16.813139964199863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "9gsyUuI-WZ3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Bidirectional,Dropout\n",
        "from keras.optimizers import Adam\n",
        "data=hourly_dataframe\n",
        "data=data.drop(columns=['date'])\n",
        "# Calculate correlation between features and target variable\n",
        "correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Define the threshold for correlation\n",
        "correlation_threshold = 0.2  # Adjust as needed\n",
        "\n",
        "# Split features into two groups based on correlation\n",
        "high_corr_features = correlation[correlation >= correlation_threshold].index.tolist()\n",
        "low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "# Define preprocessing steps for high correlation features using IterativeImputer\n",
        "high_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer(max_iter=10, random_state=42)),  # IterativeImputer for high correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "# Define preprocessing steps for low correlation features using KNNImputer\n",
        "low_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors=5)),  # KNNImputer for low correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "# Define column transformer to apply different preprocessing steps to different feature groups\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('high_corr', high_corr_transformer, high_corr_features),\n",
        "        ('low_corr', low_corr_transformer, low_corr_features)\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing steps to the data\n",
        "data_preprocessed = preprocessor.fit_transform(data)\n",
        "# Get the feature names of the transformed data\n",
        "feature_names_out = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Print the feature names\n",
        "print(feature_names_out)\n",
        "#data_preprocessed= data.values.tolist()\n",
        "\n",
        "# Rearrange data into time series format\n",
        "time_steps = 5  # Define the number of time steps\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data_preprocessed) - time_steps):\n",
        "    X.append(data_preprocessed[i:i+time_steps])\n",
        "    y.append(data_preprocessed[i+time_steps][0])\n",
        "print(y)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and testing sets (80%-20% and 90%-10%)\n",
        "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_90, X_test_10, y_train_90, y_test_10 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define the bidirectional LSTM model\n",
        "def create_bidirectional_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=64, return_sequences=True), input_shape=(X_train_80.shape[1], X_train_80.shape[2])))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(LSTM(units=64)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = create_bidirectional_lstm_model()\n",
        "model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "# Train the model (80%-20% split)\n",
        "model.fit(X_train_80, y_train_80, epochs=50, batch_size=32, verbose=1, validation_data=(X_test_20, y_test_20))\n",
        "\n",
        "# Train the model (90%-10% split)\n",
        "model.fit(X_train_90, y_train_90, epochs=50, batch_size=32, verbose=1, validation_data=(X_test_10, y_test_10))\n",
        "\n",
        "# Make predictions\n",
        "predictions_20 = model.predict(X_test_20)\n",
        "predictions_10 = model.predict(X_test_10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test_20, predictions_20)\n",
        "# Calculate MAE\n",
        "mae = np.mean(np.abs(y_test_20 - predictions_20))\n",
        "# Calculate RSE\n",
        "rse = mse / np.var(y_test_20)\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"mae:\",mae)\n",
        "print(\"rse:\",rse)\n",
        "print(\"rmse:\",rmse)\n"
      ],
      "metadata": {
        "id": "sQrT3vN9WbLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f3b6a4-a6ed-447d-d83e-b226e6926ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['high_corr__cloud_cover' 'high_corr__precipitation'\n",
            " 'high_corr__diffuse_radiation' 'low_corr__dew_point_2m'\n",
            " 'low_corr__is_day' 'low_corr__temperature_2m' 'low_corr__wind_speed_100m'\n",
            " 'low_corr__direct_radiation' 'low_corr__sunshine_duration'\n",
            " 'low_corr__surface_pressure' 'low_corr__relative_humidity_2m'\n",
            " 'low_corr__wind_direction_100m']\n",
            "[15.0, 18.0, 67.5, 60.300003, 82.200005, 19.5, 46.5, 59.100002, 13.500001, 15.6, 20.400002, 32.4, 13.200001, 13.200001, 7.8, 1.8000001, 0.6, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.5, 11.4, 6.6000004, 16.8, 5.4, 4.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8000001, 4.2000003, 3.6, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.6, 4.8, 6.6000004, 7.2000003, 0.90000004, 0.0, 76.5, 65.7, 68.1, 31.8, 6.6000004, 9.6, 4.8, 11.400001, 21.0, 9.6, 4.8, 3.6000001, 0.6, 2.4, 9.3, 4.2000003, 17.400002, 22.5, 22.500002, 28.500002, 24.300001, 21.300001, 3.9, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 9.6, 32.4, 12.0, 12.0, 26.400002, 21.0, 50.4, 6.3, 40.5, 23.400002, 29.400002, 4.8, 0.6, 0.0, 0.0, 0.0, 0.0, 0.6, 1.2, 6.3, 3.6000001, 2.7, 15.900001, 67.200005, 25.800001, 34.800003, 41.4, 43.800003, 33.0, 42.0, 27.300001, 34.2, 27.600002, 26.7, 19.2, 19.5, 29.7, 26.700003, 13.5, 21.0, 27.300001, 9.3, 12.900001, 24.6, 34.800003, 31.2, 30.900002, 20.400002, 28.2, 34.5, 41.700005, 22.5, 29.7, 26.7, 7.8, 14.1, 42.000004, 29.400002, 16.2, 0.6, 9.0, 56.7, 72.9, 19.5, 12.0, 17.400002, 9.6, 3.0, 0.0, 9.0, 2.4, 1.5, 5.4, 17.7, 27.300001, 38.4, 15.6, 9.0, 1.8000001, 3.0, 60.000004, 39.0, 9.0, 21.6, 0.0, 0.0, 0.0, 3.2999997, 0.0, 5.1, 6.3, 12.900001, 30.300001, 30.000002, 59.4, 68.100006, 64.5, 63.600002, 55.800003, 27.000002, 26.7, 5.1000004, 23.1, 2.4, 44.7, 71.700005, 70.8, 82.8, 2.4, 15.0, 18.300001, 28.2, 18.0, 58.5, 92.399994, 34.5, 18.0, 13.8, 66.3, 61.800003, 22.8, 11.4, 3.8999999, 19.800001, 51.6, 38.4, 46.800003, 58.2, 15.0, 8.400001, 4.8, 2.6999998, 0.6, 5.4, 2.7, 4.5, 5.3999996, 14.1, 11.7, 14.7, 17.7, 7.5, 24.900002, 0.3, 0.9, 16.5, 16.5, 12.0, 38.4, 39.9, 38.100002, 6.9000006, 1.2, 14.4, 12.9, 42.6, 29.7, 31.800001, 30.000002, 30.000002, 30.000002, 30.000002, 27.900002, 30.000002, 30.000002, 30.000002, 15.000001, 29.7, 30.000002, 30.000002, 30.000002, 28.800001, 8.1, 12.900001, 41.4, 66.0, 90.00001, 90.00001, 79.8, 38.100002, 17.7, 0.0, 27.0, 23.4, 0.3, 9.0, 0.0, 94.8, 52.199997, 6.2999997, 39.6, 20.699999, 24.6, 17.1, 36.0, 31.5, 17.4, 6.6000004, 1.8000001, 16.800001, 1.8000001, 0.0, 2.4, 3.6000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.7999997, 14.4, 0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.2000003, 7.8, 16.800001, 60.000004, 52.2, 21.6, 13.200001, 2.4, 1.8000001, 0.6, 0.6, 0.0, 0.0, 0.6, 1.2, 27.6, 51.900005, 25.800001, 27.6, 30.900002, 63.9, 62.400005, 60.300003, 48.3, 14.1, 20.400002, 30.900002, 4.2000003, 3.6000001, 25.2, 0.0, 0.6, 8.400001, 19.800001, 64.2, 41.1, 0.6, 0.6, 1.2, 7.8, 2.7, 3.0, 5.4, 9.3, 18.3, 63.300003, 45.3, 68.1, 45.6, 17.1, 22.2, 3.0, 35.4, 7.8, 0.0, 1.8000001, 0.0, 0.9, 11.7, 10.5, 4.5, 28.8, 42.3, 90.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.3, 6.6000004, 9.0, 12.0, 1.8000001, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.5, 4.2000003, 6.0, 6.0, 1.2, 0.6, 0.0, 0.0, 0.0, 0.0, 3.6000001, 2.4, 3.0, 10.8, 17.400002, 41.4, 53.7, 36.0, 60.000004, 60.000004, 8.400001, 0.0, 9.0, 7.2000003, 8.400001, 19.8, 18.6, 5.4, 24.0, 14.400001, 0.6, 0.6, 38.4, 22.8]\n",
            "Epoch 1/50\n",
            "14/14 [==============================] - 20s 287ms/step - loss: 600.2089 - val_loss: 434.7764\n",
            "Epoch 2/50\n",
            "14/14 [==============================] - 1s 56ms/step - loss: 493.1749 - val_loss: 359.8283\n",
            "Epoch 3/50\n",
            "14/14 [==============================] - 1s 47ms/step - loss: 437.9333 - val_loss: 347.4604\n",
            "Epoch 4/50\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 428.1756 - val_loss: 347.8550\n",
            "Epoch 5/50\n",
            "14/14 [==============================] - 1s 51ms/step - loss: 425.8594 - val_loss: 349.1559\n",
            "Epoch 6/50\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 420.6266 - val_loss: 347.2213\n",
            "Epoch 7/50\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 423.4315 - val_loss: 345.6235\n",
            "Epoch 8/50\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 422.5958 - val_loss: 345.4587\n",
            "Epoch 9/50\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 421.4505 - val_loss: 344.7921\n",
            "Epoch 10/50\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 422.7570 - val_loss: 345.7774\n",
            "Epoch 11/50\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 417.1599 - val_loss: 345.9201\n",
            "Epoch 12/50\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 411.0336 - val_loss: 344.0923\n",
            "Epoch 13/50\n",
            "14/14 [==============================] - 1s 45ms/step - loss: 413.1157 - val_loss: 345.3085\n",
            "Epoch 14/50\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 411.8884 - val_loss: 344.6009\n",
            "Epoch 15/50\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 410.6757 - val_loss: 341.7276\n",
            "Epoch 16/50\n",
            "14/14 [==============================] - 0s 36ms/step - loss: 409.2361 - val_loss: 337.7971\n",
            "Epoch 17/50\n",
            "14/14 [==============================] - 1s 44ms/step - loss: 399.9506 - val_loss: 331.8897\n",
            "Epoch 18/50\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 400.7560 - val_loss: 328.5358\n",
            "Epoch 19/50\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 390.9707 - val_loss: 338.6415\n",
            "Epoch 20/50\n",
            "14/14 [==============================] - 1s 43ms/step - loss: 393.9792 - val_loss: 322.7723\n",
            "Epoch 21/50\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 380.9437 - val_loss: 322.3562\n",
            "Epoch 22/50\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 384.2546 - val_loss: 322.3471\n",
            "Epoch 23/50\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 377.6321 - val_loss: 313.9349\n",
            "Epoch 24/50\n",
            "14/14 [==============================] - 1s 61ms/step - loss: 375.0385 - val_loss: 321.2787\n",
            "Epoch 25/50\n",
            "14/14 [==============================] - 1s 51ms/step - loss: 369.2315 - val_loss: 309.9247\n",
            "Epoch 26/50\n",
            "14/14 [==============================] - 1s 49ms/step - loss: 374.0431 - val_loss: 314.0853\n",
            "Epoch 27/50\n",
            "14/14 [==============================] - 1s 62ms/step - loss: 367.7450 - val_loss: 306.6434\n",
            "Epoch 28/50\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 366.3843 - val_loss: 301.1047\n",
            "Epoch 29/50\n",
            "14/14 [==============================] - 1s 45ms/step - loss: 362.6840 - val_loss: 329.1114\n",
            "Epoch 30/50\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 357.4355 - val_loss: 324.7740\n",
            "Epoch 31/50\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 354.8550 - val_loss: 304.1622\n",
            "Epoch 32/50\n",
            "14/14 [==============================] - 1s 41ms/step - loss: 343.3900 - val_loss: 302.1169\n",
            "Epoch 33/50\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 344.5364 - val_loss: 296.6184\n",
            "Epoch 34/50\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 338.2094 - val_loss: 306.9874\n",
            "Epoch 35/50\n",
            "14/14 [==============================] - 1s 44ms/step - loss: 330.1382 - val_loss: 294.3728\n",
            "Epoch 36/50\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 335.0425 - val_loss: 290.9752\n",
            "Epoch 37/50\n",
            "14/14 [==============================] - 1s 42ms/step - loss: 332.5578 - val_loss: 325.6621\n",
            "Epoch 38/50\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 327.6809 - val_loss: 291.2276\n",
            "Epoch 39/50\n",
            "14/14 [==============================] - 1s 71ms/step - loss: 337.7298 - val_loss: 292.9959\n",
            "Epoch 40/50\n",
            "14/14 [==============================] - 1s 49ms/step - loss: 327.3891 - val_loss: 302.1402\n",
            "Epoch 41/50\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 340.1154 - val_loss: 306.2684\n",
            "Epoch 42/50\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 337.8690 - val_loss: 303.5752\n",
            "Epoch 43/50\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 326.0339 - val_loss: 316.3551\n",
            "Epoch 44/50\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 346.5600 - val_loss: 316.0110\n",
            "Epoch 45/50\n",
            "14/14 [==============================] - 1s 45ms/step - loss: 315.1647 - val_loss: 318.8927\n",
            "Epoch 46/50\n",
            "14/14 [==============================] - 1s 47ms/step - loss: 309.2944 - val_loss: 312.3624\n",
            "Epoch 47/50\n",
            "14/14 [==============================] - 1s 60ms/step - loss: 315.1779 - val_loss: 303.1916\n",
            "Epoch 48/50\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 319.9172 - val_loss: 301.0149\n",
            "Epoch 49/50\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 306.5213 - val_loss: 294.0525\n",
            "Epoch 50/50\n",
            "14/14 [==============================] - 0s 25ms/step - loss: 308.9078 - val_loss: 299.1400\n",
            "Epoch 1/50\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 318.7605 - val_loss: 313.5918\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 306.9369 - val_loss: 301.5023\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 303.6530 - val_loss: 302.1055\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 302.6310 - val_loss: 306.3242\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 309.8401 - val_loss: 298.0996\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 318.0031 - val_loss: 279.6681\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 301.3707 - val_loss: 295.9597\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 290.0838 - val_loss: 304.1527\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 305.3981 - val_loss: 303.1439\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 295.1646 - val_loss: 294.4316\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 301.0294 - val_loss: 312.9604\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 300.9745 - val_loss: 299.3546\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 302.0304 - val_loss: 309.6086\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 285.4060 - val_loss: 313.4703\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 292.6389 - val_loss: 293.7683\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 1s 37ms/step - loss: 301.1216 - val_loss: 292.2968\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 296.7646 - val_loss: 297.7215\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 0s 28ms/step - loss: 291.5224 - val_loss: 304.6426\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 295.0427 - val_loss: 292.8123\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 280.9604 - val_loss: 297.0995\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 291.6428 - val_loss: 309.8937\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 289.3752 - val_loss: 317.1067\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 287.3347 - val_loss: 309.5730\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 280.5612 - val_loss: 284.3854\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 285.2169 - val_loss: 298.3709\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 281.2684 - val_loss: 296.3251\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 283.3618 - val_loss: 306.8125\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 275.6865 - val_loss: 290.6416\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 284.3070 - val_loss: 299.9008\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 288.6725 - val_loss: 310.8529\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 278.9177 - val_loss: 312.5703\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 290.4360 - val_loss: 293.8504\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 281.7149 - val_loss: 304.3361\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 274.3746 - val_loss: 299.0807\n",
            "Epoch 35/50\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 265.9366 - val_loss: 311.5046\n",
            "Epoch 36/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 273.4677 - val_loss: 292.3175\n",
            "Epoch 37/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 269.8826 - val_loss: 276.5808\n",
            "Epoch 38/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 277.1796 - val_loss: 305.6407\n",
            "Epoch 39/50\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 269.4858 - val_loss: 290.6614\n",
            "Epoch 40/50\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 270.9201 - val_loss: 286.5344\n",
            "Epoch 41/50\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 284.2700 - val_loss: 276.1787\n",
            "Epoch 42/50\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 286.1172 - val_loss: 300.1742\n",
            "Epoch 43/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 284.4243 - val_loss: 289.3473\n",
            "Epoch 44/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 278.3223 - val_loss: 291.6470\n",
            "Epoch 45/50\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 273.3694 - val_loss: 289.8675\n",
            "Epoch 46/50\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 280.4141 - val_loss: 304.0890\n",
            "Epoch 47/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 291.6578 - val_loss: 285.2566\n",
            "Epoch 48/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 289.1756 - val_loss: 284.7650\n",
            "Epoch 49/50\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 294.0411 - val_loss: 288.7239\n",
            "Epoch 50/50\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 274.5027 - val_loss: 288.7838\n",
            "4/4 [==============================] - 2s 9ms/step\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Mean Squared Error: 266.7489\n",
            "mae: 16.738485\n",
            "rse: 0.76746076\n",
            "rmse: 16.332449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range (len(y_test_10)):\n",
        "   print(y_test_10[i],\" \",predictions_10[i])"
      ],
      "metadata": {
        "id": "7pstexDQmfuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Better LSTM"
      ],
      "metadata": {
        "id": "y6Y2g_0_cGFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_data(data):\n",
        "    correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n",
        "    correlation_threshold = 0.1\n",
        "    high_corr_features = correlation[correlation >= correlation_threshold].index.tolist()\n",
        "    low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "    high_corr_transformer = Pipeline(steps=[\n",
        "        ('imputer', IterativeImputer(max_iter=10, random_state=42))\n",
        "    ])\n",
        "\n",
        "    low_corr_transformer = Pipeline(steps=[\n",
        "        ('imputer', KNNImputer(n_neighbors=3))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('high_corr', high_corr_transformer, high_corr_features),\n",
        "            ('low_corr', low_corr_transformer, low_corr_features)\n",
        "        ])\n",
        "\n",
        "    data_preprocessed = preprocessor.fit_transform(data)\n",
        "    return data_preprocessed, preprocessor.get_feature_names_out()\n",
        "\n",
        "# Define function to create LSTM model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=50, return_sequences=True), input_shape=input_shape))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(LSTM(units=50)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(units=2))\n",
        "    return model\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "data=hourly_dataframe\n",
        "data=data.drop(columns=['date'])\n",
        "# Load and preprocess the data\n",
        "data_preprocessed, feature_names_out = preprocess_data(data)\n",
        "\n",
        "# Rearrange data into time series format\n",
        "time_steps = 5  # Define the number of time steps\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data_preprocessed) - time_steps):\n",
        "    X.append(data_preprocessed[i:i+time_steps])\n",
        "    y.append(data_preprocessed[i+time_steps][0])\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LSTM input\n",
        "# X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "# X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
        "predictions= model.predict(X_test)\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "# Calculate MAE\n",
        "mae = np.mean(np.abs(y_test - predictions))\n",
        "# Calculate RSE\n",
        "rse = mse / np.var(y_test)\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"mae:\",mae)\n",
        "print(\"rse:\",rse)\n",
        "print(\"rmse:\",rmse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pybrzTUTW0Rz",
        "outputId": "06ce65fe-c3d8-4462-fe5c-d6f2cb60fb48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "6/6 [==============================] - 15s 398ms/step - loss: 682.1033 - val_loss: 506.6041\n",
            "Epoch 2/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 650.3189 - val_loss: 479.0123\n",
            "Epoch 3/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 619.2477 - val_loss: 450.3187\n",
            "Epoch 4/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 584.1342 - val_loss: 421.3732\n",
            "Epoch 5/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 547.3403 - val_loss: 396.0709\n",
            "Epoch 6/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 518.4625 - val_loss: 376.7141\n",
            "Epoch 7/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 494.8809 - val_loss: 364.4643\n",
            "Epoch 8/100\n",
            "6/6 [==============================] - 0s 29ms/step - loss: 478.4825 - val_loss: 357.6075\n",
            "Epoch 9/100\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 461.6167 - val_loss: 353.4958\n",
            "Epoch 10/100\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 455.2900 - val_loss: 351.2410\n",
            "Epoch 11/100\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 452.7458 - val_loss: 350.3940\n",
            "Epoch 12/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 448.9300 - val_loss: 350.2137\n",
            "Epoch 13/100\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 449.8326 - val_loss: 350.0434\n",
            "Epoch 14/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 447.0291 - val_loss: 349.4734\n",
            "Epoch 15/100\n",
            "6/6 [==============================] - 0s 28ms/step - loss: 447.0592 - val_loss: 348.2136\n",
            "Epoch 16/100\n",
            "6/6 [==============================] - 0s 36ms/step - loss: 442.1814 - val_loss: 346.9587\n",
            "Epoch 17/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 442.5875 - val_loss: 345.7223\n",
            "Epoch 18/100\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 434.8123 - val_loss: 344.3273\n",
            "Epoch 19/100\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 435.2009 - val_loss: 341.3575\n",
            "Epoch 20/100\n",
            "6/6 [==============================] - 0s 28ms/step - loss: 442.8145 - val_loss: 341.3826\n",
            "Epoch 21/100\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 434.1284 - val_loss: 343.4867\n",
            "Epoch 22/100\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 432.8573 - val_loss: 341.1392\n",
            "Epoch 23/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 434.3833 - val_loss: 341.6059\n",
            "Epoch 24/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 438.6706 - val_loss: 340.6996\n",
            "Epoch 25/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 428.0952 - val_loss: 338.0170\n",
            "Epoch 26/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 429.1331 - val_loss: 337.4895\n",
            "Epoch 27/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 425.8172 - val_loss: 337.2094\n",
            "Epoch 28/100\n",
            "6/6 [==============================] - 0s 24ms/step - loss: 426.4921 - val_loss: 334.8433\n",
            "Epoch 29/100\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 417.0958 - val_loss: 331.5778\n",
            "Epoch 30/100\n",
            "6/6 [==============================] - 0s 27ms/step - loss: 422.4616 - val_loss: 331.0226\n",
            "Epoch 31/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 407.6449 - val_loss: 322.3801\n",
            "Epoch 32/100\n",
            "6/6 [==============================] - 0s 38ms/step - loss: 409.3377 - val_loss: 316.6037\n",
            "Epoch 33/100\n",
            "6/6 [==============================] - 0s 35ms/step - loss: 403.1211 - val_loss: 312.5024\n",
            "Epoch 34/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 396.4953 - val_loss: 310.8632\n",
            "Epoch 35/100\n",
            "6/6 [==============================] - 0s 30ms/step - loss: 392.4508 - val_loss: 311.0511\n",
            "Epoch 36/100\n",
            "6/6 [==============================] - 0s 33ms/step - loss: 390.4561 - val_loss: 312.3903\n",
            "Epoch 37/100\n",
            "6/6 [==============================] - 0s 31ms/step - loss: 400.2569 - val_loss: 307.8184\n",
            "Epoch 38/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 384.8009 - val_loss: 307.3507\n",
            "Epoch 39/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 379.4306 - val_loss: 311.9002\n",
            "Epoch 40/100\n",
            "6/6 [==============================] - 0s 25ms/step - loss: 378.9371 - val_loss: 302.5845\n",
            "Epoch 41/100\n",
            "6/6 [==============================] - 0s 28ms/step - loss: 367.5407 - val_loss: 296.9978\n",
            "Epoch 42/100\n",
            "6/6 [==============================] - 0s 26ms/step - loss: 373.8649 - val_loss: 302.1944\n",
            "Epoch 43/100\n",
            "6/6 [==============================] - 0s 32ms/step - loss: 358.2661 - val_loss: 311.2398\n",
            "Epoch 44/100\n",
            "6/6 [==============================] - 0s 53ms/step - loss: 370.3546 - val_loss: 320.2647\n",
            "Epoch 45/100\n",
            "6/6 [==============================] - 0s 53ms/step - loss: 367.9597 - val_loss: 310.5517\n",
            "Epoch 46/100\n",
            "6/6 [==============================] - 0s 56ms/step - loss: 371.1509 - val_loss: 303.9169\n",
            "Epoch 47/100\n",
            "6/6 [==============================] - 0s 50ms/step - loss: 377.3401 - val_loss: 299.6743\n",
            "Epoch 48/100\n",
            "6/6 [==============================] - 0s 43ms/step - loss: 363.1745 - val_loss: 315.4475\n",
            "Epoch 49/100\n",
            "6/6 [==============================] - 0s 42ms/step - loss: 369.9828 - val_loss: 307.6723\n",
            "Epoch 50/100\n",
            "6/6 [==============================] - 0s 40ms/step - loss: 360.2594 - val_loss: 323.4519\n",
            "Epoch 51/100\n",
            "6/6 [==============================] - 0s 41ms/step - loss: 368.2253 - val_loss: 331.0613\n",
            "4/4 [==============================] - 2s 7ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "y_true and y_pred have different number of output (1!=2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1d6381037b92>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;31m# Calculate MAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;36m0.825\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    112\u001b[0m             \"y_true and y_pred have different number of output ({0}!={1})\".format(\n\u001b[1;32m    113\u001b[0m                 \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(X_test)\n",
        "for i in range (len(y_test)):\n",
        "  print(f\"{y_test[i]}   {y_pred[i]}\")"
      ],
      "metadata": {
        "id": "eYgWcbdKYBNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "rdZh175yucFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "data = hourly_dataframe  # Assuming hourly_dataframe is your dataset\n",
        "\n",
        "# Calculate correlation between features and target variable\n",
        "correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Define the threshold for correlation\n",
        "correlation_threshold = 0.1  # Adjust as needed\n",
        "\n",
        "# Split features into two groups based on correlation\n",
        "high_corr_features = correlation[correlation >= correlation_threshold].index.tolist()\n",
        "low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "# Define preprocessing steps for high correlation features using IterativeImputer\n",
        "high_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer(max_iter=10, random_state=42)),  # IterativeImputer for high correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "# Define preprocessing steps for low correlation features using KNNImputer\n",
        "low_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors=5)),  # KNNImputer for low correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "# Define column transformer to apply different preprocessing steps to different feature groups\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('high_corr', high_corr_transformer, high_corr_features),\n",
        "        ('low_corr', low_corr_transformer, low_corr_features)\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing steps to the data\n",
        "data_preprocessed = preprocessor.fit_transform(data)\n",
        "\n",
        "# Get the feature names of the transformed data\n",
        "feature_names_out = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Rearrange data into time series format\n",
        "time_steps = 5  # Define the number of time steps\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data_preprocessed) - time_steps):\n",
        "    X.append(data_preprocessed[i:i+time_steps])\n",
        "    y.append(data_preprocessed[i+time_steps][0])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and testing sets (80%-20% and 90%-10%)\n",
        "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_90, X_test_10, y_train_90, y_test_10 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "# scaler = StandardScaler()\n",
        "# X_train_80_scaled = scaler.fit_transform(X_train_80.reshape(-1, X_train_80.shape[-1])).reshape(X_train_80.shape)\n",
        "# X_test_20_scaled = scaler.transform(X_test_20.reshape(-1, X_test_20.shape[-1])).reshape(X_test_20.shape)\n",
        "# X_train_90_scaled = scaler.fit_transform(X_train_90.reshape(-1, X_train_90.shape[-1])).reshape(X_train_90.shape)\n",
        "# X_test_10_scaled = scaler.transform(X_test_10.reshape(-1, X_test_10.shape[-1])).reshape(X_test_10.shape)\n",
        "\n",
        "# Define and train the Support Vector Regressor (SVR) model (80%-20% split)\n",
        "svr_model_80 = SVR(kernel='rbf')  # RBF kernel is commonly used\n",
        "svr_model_80.fit(X_train_80.reshape(X_train_80.shape[0], -1), y_train_80)\n",
        "\n",
        "# Define and train the Support Vector Regressor (SVR) model (90%-10% split)\n",
        "svr_model_90 = SVR(kernel='rbf')  # RBF kernel is commonly used\n",
        "svr_model_90.fit(X_train_90.reshape(X_train_90.shape[0], -1), y_train_90)\n",
        "\n",
        "# Make predictions\n",
        "predictions_20 = svr_model_80.predict(X_test_20.reshape(X_test_20.shape[0], -1))\n",
        "predictions_10 = svr_model_90.predict(X_test_10.reshape(X_test_10.shape[0], -1))\n",
        "\n",
        "\n",
        "\n",
        "# Calculate Mean Squared Error for both splits\n",
        "mse_20 = mean_squared_error(y_test_20, predictions_20)\n",
        "mse_10 = mean_squared_error(y_test_10, predictions_10)\n",
        "\n",
        "print(\"Mean Squared Error (80%-20% split):\", mse_20)\n",
        "print(\"Mean Squared Error (90%-10% split):\", mse_10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOGZLscXuD7X",
        "outputId": "a451a4fe-a371-4a7e-8d36-17a2fecb6632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (80%-20% split): 414.8126584916529\n",
            "Mean Squared Error (90%-10% split): 432.38880238220963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2ca7f4eb49f9>:16: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bradient boosting"
      ],
      "metadata": {
        "id": "k-JUO7OTYoe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests_cache\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import openmeteo_requests\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define retry and cache mechanisms for API requests\n",
        "retry_strategy = Retry(\n",
        "    total=5,\n",
        "    backoff_factor=0.2,\n",
        "    status_forcelist=[429, 500, 502, 503, 504],\n",
        ")\n",
        "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
        "cache_session.mount(\"https://\", adapter)\n",
        "\n",
        "# Initialize Open-Meteo API client\n",
        "openmeteo = openmeteo_requests.Client(session=cache_session)\n",
        "\n",
        "# Make API request\n",
        "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "params = {\n",
        "    \"latitude\": 22.8101,\n",
        "    \"longitude\": 86.2634,\n",
        "    \"start_date\": \"2024-02-15\",\n",
        "    \"end_date\": \"2024-03-07\",\n",
        "    \"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \"precipitation\",\n",
        "               \"surface_pressure\", \"cloud_cover\", \"wind_speed_100m\", \"wind_direction_100m\",\n",
        "               \"is_day\", \"sunshine_duration\", \"direct_radiation\", \"diffuse_radiation\"],\n",
        "    \"timezone\": \"auto\"\n",
        "}\n",
        "response = openmeteo.weather_api(url, params=params)[0]\n",
        "\n",
        "# Extract data\n",
        "hourly = response.Hourly()\n",
        "hourly_data = {\n",
        "    # \"date\": pd.date_range(\n",
        "    #     start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
        "    #     end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
        "    #     freq=pd.Timedelta(seconds=hourly.Interval()),\n",
        "    #     closed=\"left\"\n",
        "    # ),\n",
        "    \"temperature_2m\": hourly.Variables(0).ValuesAsNumpy(),\n",
        "    \"relative_humidity_2m\": hourly.Variables(1).ValuesAsNumpy(),\n",
        "    \"dew_point_2m\": hourly.Variables(2).ValuesAsNumpy(),\n",
        "    \"precipitation\": hourly.Variables(3).ValuesAsNumpy(),\n",
        "    \"surface_pressure\": hourly.Variables(4).ValuesAsNumpy(),\n",
        "    \"cloud_cover\": hourly.Variables(5).ValuesAsNumpy(),\n",
        "    \"wind_speed_100m\": hourly.Variables(6).ValuesAsNumpy(),\n",
        "    \"wind_direction_100m\": hourly.Variables(7).ValuesAsNumpy(),\n",
        "    \"is_day\": hourly.Variables(8).ValuesAsNumpy(),\n",
        "    \"sunshine_duration\": hourly.Variables(9).ValuesAsNumpy(),\n",
        "    \"direct_radiation\": hourly.Variables(10).ValuesAsNumpy(),\n",
        "    \"diffuse_radiation\": hourly.Variables(11).ValuesAsNumpy()\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "hourly_dataframe = pd.DataFrame(hourly_data)\n",
        "\n",
        "# Convert data types\n",
        "#hourly_dataframe[\"date\"] = pd.to_datetime(hourly_dataframe[\"date\"])\n",
        "hourly_dataframe[\"is_day\"] = hourly_dataframe[\"is_day\"].astype(bool)\n",
        "hourly_dataframe = hourly_dataframe.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values\n",
        "hourly_dataframe.dropna(inplace=True)\n",
        "\n",
        "# Define features and target variable\n",
        "X = hourly_dataframe.drop(columns=[\"cloud_cover\"])\n",
        "y = hourly_dataframe[\"cloud_cover\"]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gradient Boosting Regressor model\n",
        "gbr = GradientBoostingRegressor()\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaSOcLo2YsB8",
        "outputId": "10481a6c-6313-4361-fe44-b79899ed296b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 351.17054203170875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.05, 0.1]\n",
        "}\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "# Perform Grid Search CV\n",
        "grid_search = GridSearchCV(gbr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_gbr = grid_search.best_estimator_\n",
        "\n",
        "# Train the best model\n",
        "best_gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing set\n",
        "y_pred = best_gbr.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "# Calculate MAE\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "# Calculate RSE\n",
        "rse = mse / np.var(y_test)\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"mae:\",mae)\n",
        "print(\"rse:\",rse)\n",
        "print(\"rmse:\",rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPrKwNKwZuXb",
        "outputId": "e08490f7-b6d8-4596-efc5-578134d72e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 267.13530591176016\n",
            "mae: 12.132776200774352\n",
            "rse: 0.6221868608506471\n",
            "rmse: 16.34427440762545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Only correlated features"
      ],
      "metadata": {
        "id": "Rixgz7wSa4wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Bidirectional,Dropout\n",
        "from keras.optimizers import Adam\n",
        "data=hourly_dataframe\n",
        "\n",
        "# Calculate correlation between features and target variable\n",
        "correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Define the threshold for correlation\n",
        "correlation_threshold = 0.1 # Adjust as needed\n",
        "\n",
        "# Split features into two groups based on correlation\n",
        "high_corr_features = correlation[correlation >= correlation_threshold].index.tolist()\n",
        "low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "\n",
        "# Define preprocessing steps for high correlation features using IterativeImputer\n",
        "high_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer(max_iter=10, random_state=42)),  # IterativeImputer for high correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('high_corr', high_corr_transformer, high_corr_features),\n",
        "        ('low_corr', 'drop', low_corr_features)  # Drop low correlated features\n",
        "    ],\n",
        "    remainder='passthrough'  # Include any remaining columns not specified in transformers\n",
        ")\n",
        "# Apply preprocessing steps to the data\n",
        "data_preprocessed = preprocessor.fit_transform(data)\n",
        "# Get the feature names of the transformed data\n",
        "feature_names_out = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Print the feature names\n",
        "print(feature_names_out)\n",
        "\n",
        "\n",
        "# Rearrange data into time series format\n",
        "time_steps = 5  # Define the number of time steps\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data_preprocessed) - time_steps):\n",
        "    X.append(data_preprocessed[i:i+time_steps])\n",
        "    y.append(data_preprocessed[i+time_steps][0])\n",
        "print(y)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and testing sets (80%-20% and 90%-10%)\n",
        "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the bidirectional LSTM model\n",
        "def create_bidirectional_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=32, return_sequences=True), input_shape=(X_train_80.shape[1], X_train_80.shape[2])))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(LSTM(units=32)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = create_bidirectional_lstm_model()\n",
        "model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "# Train the model (80%-20% split)\n",
        "model.fit(X_train_80, y_train_80, epochs=50, batch_size=32, verbose=1, validation_data=(X_test_20, y_test_20))\n",
        "model.save(\"your_model_name.h5\")\n",
        "#To load tf.keras.models.load_model()\n",
        "\n",
        "# Make predictions\n",
        "predictions_20 = model.predict(X_test_20)\n",
        "\n",
        "\n",
        "# Calculate Mean Squared Error for both splits\n",
        "mse_20 = mean_squared_error(y_test_20, predictions_20)\n",
        "\n",
        "print(\"Mean Squared Error (80%-20% split):\", mse_20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "ef0f49b0-b5ab-422f-b40b-eeb08246f8fe",
        "id": "fHcrlabka8w0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['remainder__temperature_2m' 'remainder__dew_point_2m'\n",
            " 'remainder__precipitation' 'remainder__cloud_cover' 'remainder__is_day'\n",
            " 'remainder__diffuse_radiation']\n",
            "[16.857500076293945, 18.957500457763672, 19.457500457763672, 20.3075008392334, 21.907499313354492, 20.957500457763672, 23.407499313354492, 25.207500457763672, 25.457500457763672, 26.5575008392334, 26.357500076293945, 26.0575008392334, 23.857500076293945, 20.0575008392334, 20.607500076293945, 19.5575008392334, 19.00749969482422, 18.457500457763672, 18.707500457763672, 16.707500457763672, 16.907499313354492, 16.207500457763672, 16.00749969482422, 15.507499694824219, 15.557499885559082, 17.407499313354492, 19.0575008392334, 21.457500457763672, 23.657499313354492, 25.50749969482422, 26.8075008392334, 27.457500457763672, 27.907499313354492, 28.0575008392334, 27.957500457763672, 27.3075008392334, 27.3075008392334, 25.50749969482422, 21.75749969482422, 19.457500457763672, 18.657499313354492, 17.657499313354492, 16.707500457763672, 16.157499313354492, 15.057499885559082, 14.807499885559082, 14.807499885559082, 14.557499885559082, 13.507499694824219, 15.407500267028809, 17.957500457763672, 21.357500076293945, 23.657499313354492, 25.157499313354492, 26.25749969482422, 27.357500076293945, 28.457500457763672, 28.207500457763672, 27.907499313354492, 27.357500076293945, 23.50749969482422, 21.5575008392334, 22.407499313354492, 19.5575008392334, 18.8075008392334, 18.407499313354492, 16.907499313354492, 16.307498931884766, 15.757499694824219, 15.157500267028809, 14.507499694824219, 13.557499885559082, 13.307499885559082, 14.757499694824219, 17.50749969482422, 20.707500457763672, 23.107500076293945, 25.3075008392334, 27.0575008392334, 28.207500457763672, 29.107500076293945, 29.25749969482422, 28.5575008392334, 28.00749969482422, 25.107500076293945, 23.357500076293945, 21.75749969482422, 21.0575008392334, 20.407499313354492, 19.25749969482422, 19.50749969482422, 18.607500076293945, 17.607500076293945, 17.207500457763672, 16.707500457763672, 16.457500457763672, 15.557499885559082, 18.3075008392334, 20.207500457763672, 22.207500457763672, 24.157499313354492, 26.0575008392334, 27.707500457763672, 28.957500457763672, 30.00749969482422, 30.207500457763672, 30.157499313354492, 29.25749969482422, 28.707500457763672, 24.707500457763672, 23.00749969482422, 22.207500457763672, 21.457500457763672, 22.657499313354492, 22.357500076293945, 22.207500457763672, 21.957500457763672, 18.957500457763672, 18.0575008392334, 17.75749969482422, 17.407499313354492, 19.907499313354492, 21.8075008392334, 24.457500457763672, 27.107500076293945, 29.207500457763672, 31.0575008392334, 31.907499313354492, 32.557498931884766, 32.50749969482422, 32.457496643066406, 30.657499313354492, 28.25749969482422, 25.8075008392334, 23.75749969482422, 24.8075008392334, 24.107500076293945, 24.957500457763672, 23.407499313354492, 20.157499313354492, 19.50749969482422, 18.857500076293945, 17.957500457763672, 17.3075008392334, 19.407499313354492, 18.8075008392334, 21.407499313354492, 24.157499313354492, 26.857500076293945, 29.207500457763672, 29.707500457763672, 31.25749969482422, 31.107500076293945, 31.157499313354492, 31.407499313354492, 30.157499313354492, 28.607500076293945, 24.3075008392334, 23.75749969482422, 23.50749969482422, 23.707500457763672, 23.3075008392334, 21.157499313354492, 20.107500076293945, 20.50749969482422, 20.00749969482422, 19.407499313354492, 20.75749969482422, 18.25749969482422, 19.207500457763672, 21.50749969482422, 24.5575008392334, 27.50749969482422, 29.00749969482422, 30.357500076293945, 30.8075008392334, 31.25749969482422, 32.50749969482422, 31.75749969482422, 30.607500076293945, 28.907499313354492, 24.407499313354492, 23.0575008392334, 22.857500076293945, 22.657499313354492, 21.5575008392334, 20.75749969482422, 20.357500076293945, 20.357500076293945, 19.75749969482422, 19.0575008392334, 18.5575008392334, 18.00749969482422, 18.857500076293945, 20.75749969482422, 22.957500457763672, 24.75749969482422, 26.207500457763672, 27.407499313354492, 28.25749969482422, 28.5575008392334, 28.00749969482422, 27.50749969482422, 26.957500457763672, 25.25749969482422, 25.357500076293945, 25.157499313354492, 22.707500457763672, 21.607500076293945, 20.657499313354492, 20.407499313354492, 19.657499313354492, 18.707500457763672, 18.00749969482422, 17.357500076293945, 16.207500457763672, 16.25749969482422, 18.107500076293945, 19.5575008392334, 21.5575008392334, 22.907499313354492, 23.75749969482422, 23.457500457763672, 23.357500076293945, 23.50749969482422, 23.107500076293945, 22.607500076293945, 22.707500457763672, 21.407499313354492, 23.157499313354492, 22.0575008392334, 21.8075008392334, 21.657499313354492, 19.707500457763672, 18.857500076293945, 18.5575008392334, 17.8075008392334, 17.25749969482422, 17.207500457763672, 16.907499313354492, 17.357500076293945, 19.00749969482422, 19.8075008392334, 21.0575008392334, 23.25749969482422, 22.657499313354492, 23.107500076293945, 25.50749969482422, 26.207500457763672, 26.607500076293945, 26.50749969482422, 25.907499313354492, 24.357500076293945, 23.457500457763672, 23.107500076293945, 21.957500457763672, 21.00749969482422, 20.357500076293945, 19.75749969482422, 19.107500076293945, 18.457500457763672, 18.00749969482422, 17.857500076293945, 17.50749969482422, 17.25749969482422, 18.607500076293945, 20.50749969482422, 22.5575008392334, 24.207500457763672, 25.857500076293945, 27.25749969482422, 28.357500076293945, 28.607500076293945, 28.907499313354492, 29.00749969482422, 28.457500457763672, 27.5575008392334, 23.907499313354492, 22.957500457763672, 22.50749969482422, 22.357500076293945, 21.00749969482422, 20.657499313354492, 20.107500076293945, 19.8075008392334, 19.607500076293945, 19.407499313354492, 18.75749969482422, 18.157499313354492, 19.107500076293945, 21.00749969482422, 23.157499313354492, 25.00749969482422, 26.75749969482422, 27.75749969482422, 28.5575008392334, 29.357500076293945, 29.457500457763672, 29.707500457763672, 29.407499313354492, 27.5575008392334, 22.107500076293945, 21.707500457763672, 21.607500076293945, 21.50749969482422, 21.157499313354492, 20.25749969482422, 19.25749969482422, 19.5575008392334, 19.457500457763672, 19.25749969482422, 18.707500457763672, 18.5575008392334, 19.357500076293945, 20.107500076293945, 22.25749969482422, 24.957500457763672, 26.907499313354492, 27.907499313354492, 28.457500457763672, 29.157499313354492, 28.957500457763672, 29.3075008392334, 28.75749969482422, 28.107500076293945, 26.107500076293945, 24.457500457763672, 22.407499313354492, 21.607500076293945, 21.3075008392334, 20.657499313354492, 19.75749969482422, 19.157499313354492, 17.8075008392334, 17.00749969482422, 16.607500076293945, 16.00749969482422, 17.957500457763672, 20.75749969482422, 23.8075008392334, 26.00749969482422, 27.657499313354492, 28.75749969482422, 29.657499313354492, 30.157499313354492, 30.3075008392334, 29.957500457763672, 29.207500457763672, 26.25749969482422, 23.857500076293945, 23.357500076293945, 22.157499313354492, 21.3075008392334, 19.907499313354492, 19.457500457763672, 17.907499313354492, 17.25749969482422, 16.8075008392334, 16.607500076293945, 16.057498931884766, 15.257499694824219, 17.457500457763672, 19.907499313354492, 22.907499313354492, 25.657499313354492, 27.707500457763672, 29.0575008392334, 30.0575008392334, 30.75749969482422, 31.00749969482422, 30.107500076293945, 29.457500457763672, 26.3075008392334, 27.00749969482422, 23.3075008392334, 25.407499313354492, 25.707500457763672, 20.5575008392334, 19.207500457763672, 18.75749969482422, 19.157499313354492, 18.25749969482422, 17.657499313354492, 16.50749969482422, 17.50749969482422, 17.8075008392334, 20.75749969482422, 24.25749969482422, 26.75749969482422, 28.707500457763672, 30.25749969482422, 31.357500076293945, 32.057498931884766, 32.25749969482422, 31.957500457763672, 31.00749969482422, 30.207500457763672, 28.00749969482422, 25.5575008392334, 25.657499313354492, 24.25749969482422, 23.457500457763672, 22.207500457763672, 20.907499313354492, 20.907499313354492, 20.8075008392334, 20.157499313354492, 19.5575008392334, 20.857500076293945, 20.407499313354492, 23.00749969482422, 25.25749969482422, 27.457500457763672, 28.957500457763672, 30.00749969482422, 30.357500076293945, 30.407499313354492, 30.157499313354492, 30.207500457763672, 29.957500457763672, 27.607500076293945, 24.957500457763672, 23.8075008392334, 23.407499313354492, 23.25749969482422, 22.3075008392334, 21.50749969482422, 21.25749969482422, 21.0575008392334, 20.657499313354492, 20.407499313354492, 19.657499313354492, 18.75749969482422, 20.3075008392334, 21.657499313354492, 24.107500076293945, 26.357500076293945, 27.957500457763672, 29.5575008392334, 28.8075008392334, 28.0575008392334, 28.407499313354492, 28.457500457763672, 27.957500457763672, 26.207500457763672, 22.607500076293945, 21.957500457763672, 21.50749969482422, 20.957500457763672, 21.5575008392334, 19.8075008392334, 19.407499313354492, 19.50749969482422, 19.157499313354492, 19.207500457763672, 19.207500457763672, 18.907499313354492, 20.3075008392334, 21.75749969482422, 24.3075008392334, 26.457500457763672, 27.957500457763672, 29.207500457763672, 30.107500076293945, 30.207500457763672, 30.157499313354492, 30.3075008392334, 30.00749969482422, 29.5575008392334, 26.857500076293945, 22.607500076293945, 21.607500076293945, 20.607500076293945, 19.75749969482422, 19.157499313354492, 18.707500457763672, 17.8075008392334, 17.25749969482422, 17.5575008392334, 17.657499313354492, 17.50749969482422, 19.00749969482422, 21.0575008392334, 23.25749969482422, 25.25749969482422, 27.00749969482422, 28.407499313354492, 29.457500457763672, 30.3075008392334, 30.907499313354492, 30.907499313354492, 29.707500457763672, 26.5575008392334, 26.3075008392334, 24.00749969482422, 22.8075008392334, 21.5575008392334, 20.657499313354492, 20.457500457763672, 20.5575008392334, 19.707500457763672, 19.407499313354492, 19.25749969482422, 19.157499313354492, 18.50749969482422, 20.207500457763672, 20.8075008392334, 22.25749969482422, 23.207500457763672, 24.207500457763672, 26.707500457763672, 28.3075008392334, 29.357500076293945, 29.857500076293945, 29.8075008392334, 29.107500076293945, 26.75749969482422, 24.407499313354492, 23.607500076293945, 22.957500457763672, 21.457500457763672, 20.957500457763672, 21.657499313354492]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-246bdc704160>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Train the model (80%-20% split)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"your_model_name.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#To load tf.keras.models.load_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(predictions_20)):\n",
        "  print(f\"{y_test_20[i]}  {predictions_20[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYeSMLr7d8cr",
        "outputId": "03c45a87-9b94-4737-c016-4c9847b0e2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0  [-0.01779559]\n",
            "0.0  [0.00345083]\n",
            "0.0  [0.04938785]\n",
            "0.0  [-0.03089701]\n",
            "0.0  [0.00909953]\n",
            "0.0  [0.01538204]\n",
            "0.0  [0.03834464]\n",
            "0.0  [0.00794513]\n",
            "0.0  [-0.00154605]\n",
            "0.0  [0.04684485]\n",
            "0.0  [0.01357909]\n",
            "0.0  [0.03660073]\n",
            "0.0  [-0.0038519]\n",
            "0.0  [-0.01031295]\n",
            "0.0  [0.01055505]\n",
            "0.0  [0.01125633]\n",
            "0.0  [0.05045082]\n",
            "0.0  [0.0037007]\n",
            "0.0  [0.00776191]\n",
            "0.0  [0.01050768]\n",
            "0.0  [-0.06926161]\n",
            "0.0  [0.01327034]\n",
            "0.0  [-0.03206972]\n",
            "0.0  [0.0533639]\n",
            "0.0  [-0.00294903]\n",
            "0.0  [-0.03286945]\n",
            "0.0  [0.01019238]\n",
            "0.0  [-0.02745265]\n",
            "0.0  [0.04095551]\n",
            "0.0  [0.00477759]\n",
            "0.0  [-0.02477651]\n",
            "0.0  [0.02979022]\n",
            "0.0  [0.00988598]\n",
            "0.0  [0.01055505]\n",
            "0.10000000149011612  [0.00983606]\n",
            "0.0  [0.01055505]\n",
            "0.0  [0.01386825]\n",
            "0.0  [0.01055505]\n",
            "0.0  [0.0077253]\n",
            "0.0  [0.02901131]\n",
            "0.0  [0.01595106]\n",
            "0.0  [0.03077976]\n",
            "0.0  [0.03336083]\n",
            "0.0  [0.00238397]\n",
            "0.0  [-0.01361361]\n",
            "0.0  [0.0797815]\n",
            "0.0  [0.01055505]\n",
            "0.0  [-0.01764097]\n",
            "0.0  [0.01055505]\n",
            "0.0  [-0.00144126]\n",
            "0.0  [-0.06560966]\n",
            "0.0  [0.00866807]\n",
            "0.0  [-0.0385942]\n",
            "0.0  [0.01017092]\n",
            "0.0  [0.0203248]\n",
            "0.0  [0.00392861]\n",
            "0.0  [0.00164513]\n",
            "0.0  [-0.01036767]\n",
            "0.0  [-0.02926775]\n",
            "0.0  [0.00875557]\n",
            "0.0  [0.00866057]\n",
            "1.7999999523162842  [0.5692181]\n",
            "0.0  [0.00661467]\n",
            "0.0  [0.02163225]\n",
            "0.0  [0.01055505]\n",
            "0.0  [0.01066847]\n",
            "0.0  [-0.013171]\n",
            "0.0  [0.00902357]\n",
            "0.0  [-0.00516216]\n",
            "0.0  [-0.00419395]\n",
            "0.0  [0.01459826]\n",
            "0.0  [-0.00143874]\n",
            "0.0  [0.00052067]\n",
            "0.0  [0.01643193]\n",
            "0.0  [-0.00279873]\n",
            "0.0  [0.01090301]\n",
            "0.0  [0.05035302]\n",
            "0.0  [0.01205248]\n",
            "0.0  [0.01272332]\n",
            "0.0  [0.00970067]\n",
            "0.0  [0.03009388]\n",
            "0.0  [0.00252796]\n",
            "0.0  [-0.03617226]\n",
            "0.0  [0.00959117]\n",
            "0.0  [0.04015101]\n",
            "0.0  [0.11796011]\n",
            "0.0  [0.00477679]\n",
            "0.0  [-0.04495804]\n",
            "0.0  [0.00949568]\n",
            "0.0  [-0.05033289]\n",
            "0.0  [0.09156464]\n",
            "0.0  [-0.00362445]\n",
            "0.0  [0.00577663]\n",
            "0.0  [0.01055505]\n",
            "0.0  [0.0201866]\n",
            "0.0  [0.01325649]\n",
            "0.0  [0.01863634]\n",
            "0.0  [0.0144936]\n",
            "0.0  [0.0040654]\n",
            "0.0  [0.01452289]\n",
            "0.0  [0.01055505]\n",
            "0.0  [0.01219724]\n",
            "3.0999999046325684  [0.0550895]\n",
            "0.30000001192092896  [0.08406919]\n",
            "0.0  [0.01528903]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "data = hourly_dataframe  # Assuming hourly_dataframe is your dataset\n",
        "\n",
        "# Calculate correlation between features and target variable\n",
        "correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Define the threshold for correlation\n",
        "correlation_threshold = 0.1  # Adjust as needed\n",
        "\n",
        "# Split features into two groups based on correlation\n",
        "high_corr_features = correlation[correlation >= correlation_threshold].index.tolist()\n",
        "low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "# Define preprocessing steps for high correlation features using IterativeImputer\n",
        "high_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer(max_iter=10, random_state=42)),  # IterativeImputer for high correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "\n",
        "# Define column transformer to apply different preprocessing steps to different feature groups\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('high_corr', high_corr_transformer, high_corr_features),\n",
        "        ('low_corr', 'drop', low_corr_features)\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing steps to the data\n",
        "data_preprocessed = preprocessor.fit_transform(data)\n",
        "\n",
        "# Get the feature names of the transformed data\n",
        "feature_names_out = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Rearrange data into time series format\n",
        "time_steps = 2  # Define the number of time steps\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data_preprocessed) - time_steps):\n",
        "    X.append(data_preprocessed[i:i+time_steps])\n",
        "    y.append(data_preprocessed[i+time_steps][0])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and testing sets (80%-20% and 90%-10%)\n",
        "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_90, X_test_10, y_train_90, y_test_10 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "#Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_80_scaled = scaler.fit_transform(X_train_80.reshape(-1, X_train_80.shape[-1])).reshape(X_train_80.shape)\n",
        "X_test_20_scaled = scaler.transform(X_test_20.reshape(-1, X_test_20.shape[-1])).reshape(X_test_20.shape)\n",
        "X_train_90_scaled = scaler.fit_transform(X_train_90.reshape(-1, X_train_90.shape[-1])).reshape(X_train_90.shape)\n",
        "X_test_10_scaled = scaler.transform(X_test_10.reshape(-1, X_test_10.shape[-1])).reshape(X_test_10.shape)\n",
        "\n",
        "# Define and train the Support Vector Regressor (SVR) model (80%-20% split)\n",
        "svr_model_80 = SVR(kernel='rbf')  # RBF kernel is commonly used\n",
        "svr_model_80.fit(X_train_80.reshape(X_train_80.shape[0], -1), y_train_80)\n",
        "\n",
        "# Define and train the Support Vector Regressor (SVR) model (90%-10% split)\n",
        "svr_model_90 = SVR(kernel='rbf')  # RBF kernel is commonly used\n",
        "svr_model_90.fit(X_train_90.reshape(X_train_90.shape[0], -1), y_train_90)\n",
        "\n",
        "# Make predictions\n",
        "predictions_20 = svr_model_80.predict(X_test_20.reshape(X_test_20.shape[0], -1))\n",
        "predictions_10 = svr_model_90.predict(X_test_10.reshape(X_test_10.shape[0], -1))\n",
        "\n",
        "\n",
        "\n",
        "# Calculate Mean Squared Error for both splits\n",
        "mse_20 = mean_squared_error(y_test_20, predictions_20)\n",
        "mse_10 = mean_squared_error(y_test_10, predictions_10)\n",
        "\n",
        "print(\"Mean Squared Error (80%-20% split):\", mse_20)\n",
        "print(\"Mean Squared Error (90%-10% split):\", mse_10)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = np.mean(np.abs(y_test_20 - predictions_20))\n",
        "# Calculate RSE\n",
        "rse = mse / np.var(y_test_20)\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse_20)\n",
        "print(\"Mean Squared Error:\", mse_20)\n",
        "print(\"mae:\",mae)\n",
        "print(\"rse:\",rse)\n",
        "print(\"rmse:\",rmse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c38f2d05-686c-4819-fed9-ea544bc0e05e",
        "id": "aZIbWteleClo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (80%-20% split): 304.74293736260034\n",
            "Mean Squared Error (90%-10% split): 275.7221567924782\n",
            "Mean Squared Error: 304.74293736260034\n",
            "mae: 10.727742196068093\n",
            "rse: 0.7528117\n",
            "rmse: 17.456887963282583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PCEWxL9PfOMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "data=hourly_dataframe\n",
        "# Load the dataset\n",
        "#data = pd.read_csv('your_dataset.csv')  # Replace 'your_dataset.csv' with the actual filename\n",
        "\n",
        "# Calculate correlation between features and target variable\n",
        "correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Define the threshold for correlation\n",
        "correlation_threshold = 0.1  # Adjust as needed\n",
        "\n",
        "# Split features into two groups based on correlation\n",
        "high_corr_features = correlation[correlation >= correlation_threshold]\n",
        "high_corr_features=high_corr_features.drop('cloud_cover')\n",
        "high_corr_features = high_corr_features.index.tolist()\n",
        "low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "# # Prepare the data for imputation\n",
        "y = data['cloud_cover']\n",
        "X = data.drop(columns=['cloud_cover'])\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define preprocessing steps for high correlation features using IterativeImputer\n",
        "high_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer(max_iter=20, random_state=42)),  # IterativeImputer for high correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "# Define column transformer to apply different preprocessing steps to different feature groups\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('high_corr', high_corr_transformer, high_corr_features)\n",
        "        ,('low_corr', 'drop', low_corr_features)\n",
        "    ])\n",
        "\n",
        "preprocessor.fit(X_train,y_train)\n",
        "# Define the model\n",
        "model = RandomForestRegressor()\n",
        "\n",
        "# Create a pipeline with preprocessing and modeling steps\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', model)])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "#model.fit(X_train,y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = pipeline.predict(X_test)\n",
        "#predictions=model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3300ab94-d56b-4b78-a06b-77d46d6cc2cc",
        "id": "b1LfE9IjfOl1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 328.4602858344121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_data(data):\n",
        "    correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n",
        "    correlation_threshold = 0.1\n",
        "    high_corr_features = correlation[correlation >= correlation_threshold].index.tolist()\n",
        "    low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "    high_corr_transformer = Pipeline(steps=[\n",
        "        ('imputer', IterativeImputer(max_iter=10, random_state=42))\n",
        "    ])\n",
        "\n",
        "    low_corr_transformer = Pipeline(steps=[\n",
        "        ('imputer', KNNImputer(n_neighbors=3))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('high_corr', high_corr_transformer, high_corr_features),\n",
        "            ('low_corr', 'drop', low_corr_features)\n",
        "        ])\n",
        "\n",
        "    data_preprocessed = preprocessor.fit_transform(data)\n",
        "    return data_preprocessed, preprocessor.get_feature_names_out()\n",
        "\n",
        "# Define function to create LSTM model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=50, return_sequences=True), input_shape=input_shape))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(LSTM(units=50)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(units=2))\n",
        "    return model\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "# Load and preprocess the data\n",
        "data_preprocessed, feature_names_out = preprocess_data(hourly_dataframe)\n",
        "\n",
        "# Rearrange data into time series format\n",
        "time_steps = 5  # Define the number of time steps\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data_preprocessed) - time_steps):\n",
        "    X.append(data_preprocessed[i:i+time_steps])\n",
        "    y.append(data_preprocessed[i+time_steps][0])\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data for LSTM input\n",
        "# X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "# X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "mse = model.evaluate(X_test, y_test)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n"
      ],
      "metadata": {
        "id": "8YPTQI3ehHK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Model"
      ],
      "metadata": {
        "id": "kwn430AV5MAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Bidirectional,Dropout\n",
        "from keras.optimizers import Adam\n",
        "data=hourly_dataframe\n",
        "\n",
        "# Calculate correlation between features and target variable\n",
        "correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Define the threshold for correlation\n",
        "correlation_threshold = 0.1 # Adjust as needed\n",
        "\n",
        "# Split features into two groups based on correlation\n",
        "high_corr_features = correlation[correlation >= correlation_threshold].index.tolist()\n",
        "low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "\n",
        "# Define preprocessing steps for high correlation features using IterativeImputer\n",
        "high_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer(max_iter=10, random_state=42)),  # IterativeImputer for high correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('high_corr', high_corr_transformer, high_corr_features),\n",
        "        ('low_corr', 'drop', low_corr_features)  # Drop low correlated features\n",
        "    ],\n",
        "    remainder='passthrough'  # Include any remaining columns not specified in transformers\n",
        ")\n",
        "# Apply preprocessing steps to the data\n",
        "data_preprocessed = preprocessor.fit_transform(data)\n",
        "# Get the feature names of the transformed data\n",
        "feature_names_out = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Print the feature names\n",
        "print(feature_names_out)\n",
        "\n",
        "\n",
        "# Rearrange data into time series format\n",
        "time_steps = 5  # Define the number of time steps\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data_preprocessed) - time_steps):\n",
        "    X.append(data_preprocessed[i:i+time_steps])\n",
        "    y.append(data_preprocessed[i+time_steps][0])\n",
        "print(y)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and testing sets (80%-20% and 90%-10%)\n",
        "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the bidirectional LSTM model\n",
        "def create_bidirectional_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=32, return_sequences=True), input_shape=(X_train_80.shape[1], X_train_80.shape[2])))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(LSTM(units=32)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1))\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = create_bidirectional_lstm_model()\n",
        "model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "# Train the model (80%-20% split)\n",
        "model.fit(X_train_80, y_train_80, epochs=50, batch_size=32, verbose=1, validation_data=(X_test_20, y_test_20))\n",
        "model.save(\"your_model_name.h5\")\n",
        "#To load tf.keras.models.load_model()\n",
        "\n",
        "# Make predictions\n",
        "predictions_20 = model.predict(X_test_20)\n",
        "\n",
        "\n",
        "# Calculate Mean Squared Error for both splits\n",
        "mse_20 = mean_squared_error(y_test_20, predictions_20)\n",
        "\n",
        "print(\"Mean Squared Error (80%-20% split):\", mse_20)\n"
      ],
      "metadata": {
        "id": "hd1AaymJ5PPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.constraints import non_neg\n",
        "from keras import backend as K\n",
        "\n",
        "# Assuming 'hourly_dataframe' is your dataset\n",
        "data = hourly_dataframe.copy()\n",
        "data=data.drop(columns=['date'])\n",
        "# Handling Missing Values\n",
        "# You can choose either IterativeImputer or KNNImputer\n",
        "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
        "data_imputed = imputer.fit_transform(data)\n",
        "\n",
        "# Convert the imputed data back to a DataFrame\n",
        "data_imputed_df = pd.DataFrame(data_imputed, columns=data.columns)\n",
        "\n",
        "# Calculate correlation between features and target variable\n",
        "correlation = np.abs(data_imputed_df.corr()['cloud_cover']).sort_values(ascending=False)\n",
        "\n",
        "# Define the threshold for correlation\n",
        "correlation_threshold = 0.1 # Adjust as needed\n",
        "\n",
        "# Split features into two groups based on correlation\n",
        "high_corr_features = correlation[correlation >= correlation_threshold].index.tolist()\n",
        "low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "# Define the ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('low_corr', 'drop', low_corr_features)  # Drop low correlated features\n",
        "    ],\n",
        "    remainder='passthrough'  # Include any remaining columns not specified in transformers\n",
        ")\n",
        "\n",
        "# Apply preprocessing steps to the data\n",
        "data_preprocessed = preprocessor.fit_transform(data_imputed_df)\n",
        "# Get the feature names of the transformed data\n",
        "feature_names_out = preprocessor.get_feature_names_out()\n",
        "print(feature_names_out)\n",
        "# Rearrange data into time series format\n",
        "time_steps = 2 # Define the number of time steps\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data_preprocessed) - time_steps):\n",
        "    X.append(data_preprocessed[i:i+time_steps])\n",
        "    y.append(data_preprocessed[i+time_steps][3])  # Assuming index 0 is the target variable\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# Split the data into training and testing sets (80%-20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=32)#42#32\n",
        "\n",
        "\n",
        "# Define the bidirectional LSTM model\n",
        "def create_bidirectional_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(units=84, return_sequences=True), input_shape=input_shape))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Bidirectional(LSTM(units=84)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1,kernel_constraint=non_neg()))\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "print(X_train)\n",
        "print(X_train.shape[1],',', X_train.shape[2])\n",
        "model = create_bidirectional_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "model.compile(optimizer=Adam(),loss='mean_squared_error')\n",
        "\n",
        "# Define a model checkpoint callback to save the model with the lowest validation loss\n",
        "checkpoint_path = \"best_model.h5\"\n",
        "model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# Train the model with the checkpoint callback\n",
        "history = model.fit(X_train, y_train, epochs=32, batch_size=8, verbose=1, validation_data=(X_test, y_test), callbacks=[model_checkpoint])\n",
        "\n",
        "# Load the best model from the checkpoint\n",
        "best_model = load_model(checkpoint_path)\n",
        "\n",
        "# Make predictions with the best model\n",
        "predictions = best_model.predict(X_test)\n",
        "\n",
        "# Ensure predictions are non-negative\n",
        "predictions = np.maximum(predictions, 0.0)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "# Calculate MAE\n",
        "mae = np.mean(np.abs(y_test - predictions))\n",
        "# Calculate RSE\n",
        "rse = mse / np.var(y_test)\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"mae:\",mae)\n",
        "print(\"rse:\",rse)\n",
        "print(\"rmse:\",rmse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F36oM_M78H5d",
        "outputId": "086c6332-2fb2-4073-c55b-fd9c1b7bcc84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['remainder__temperature_2m' 'remainder__dew_point_2m'\n",
            " 'remainder__precipitation' 'remainder__cloud_cover' 'remainder__is_day'\n",
            " 'remainder__diffuse_radiation']\n",
            "[[[ 19.5075    13.2575     0.         0.         0.         0.      ]\n",
            "  [ 18.8575    13.6075     0.         0.         0.         0.      ]]\n",
            "\n",
            " [[ 27.9075    12.8075     0.         3.6        1.       126.      ]\n",
            "  [ 27.3575    14.7075     0.         0.6        1.        95.      ]]\n",
            "\n",
            " [[ 30.3575    15.8075     0.        62.400005   1.       358.      ]\n",
            "  [ 30.4075    15.9575     0.        60.300003   1.       315.      ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 25.5575    15.1575     0.        52.2        0.         0.      ]\n",
            "  [ 25.6575    14.2575     0.        21.6        0.         0.      ]]\n",
            "\n",
            " [[ 28.0075    15.2075     0.         0.         1.        94.      ]\n",
            "  [ 25.1075    15.6075     0.         0.6        1.        44.      ]]\n",
            "\n",
            " [[ 21.1575    20.3575     1.2       38.100002   0.         0.      ]\n",
            "  [ 20.2575    19.8575     0.        17.7        0.         0.      ]]]\n",
            "2 , 6\n",
            "Epoch 1/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 560.5663\n",
            "Epoch 1: val_loss improved from inf to 186.64204, saving model to best_model.h5\n",
            "60/60 [==============================] - 23s 104ms/step - loss: 559.6471 - val_loss: 186.6420\n",
            "Epoch 2/32\n",
            " 4/60 [=>............................] - ETA: 1s - loss: 333.7626"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58/60 [============================>.] - ETA: 0s - loss: 387.7594\n",
            "Epoch 2: val_loss improved from 186.64204 to 132.65544, saving model to best_model.h5\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 381.2404 - val_loss: 132.6554\n",
            "Epoch 3/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 319.1494\n",
            "Epoch 3: val_loss improved from 132.65544 to 131.66283, saving model to best_model.h5\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 319.4528 - val_loss: 131.6628\n",
            "Epoch 4/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 298.4806\n",
            "Epoch 4: val_loss improved from 131.66283 to 128.78586, saving model to best_model.h5\n",
            "60/60 [==============================] - 1s 23ms/step - loss: 298.0280 - val_loss: 128.7859\n",
            "Epoch 5/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 278.3974\n",
            "Epoch 5: val_loss did not improve from 128.78586\n",
            "60/60 [==============================] - 1s 21ms/step - loss: 278.3974 - val_loss: 143.1097\n",
            "Epoch 6/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 269.4718\n",
            "Epoch 6: val_loss did not improve from 128.78586\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 269.4718 - val_loss: 132.0136\n",
            "Epoch 7/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 258.4029\n",
            "Epoch 7: val_loss improved from 128.78586 to 120.84196, saving model to best_model.h5\n",
            "60/60 [==============================] - 2s 40ms/step - loss: 258.4029 - val_loss: 120.8420\n",
            "Epoch 8/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 247.2850\n",
            "Epoch 8: val_loss did not improve from 120.84196\n",
            "60/60 [==============================] - 2s 40ms/step - loss: 249.1453 - val_loss: 129.6101\n",
            "Epoch 9/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 244.6223\n",
            "Epoch 9: val_loss improved from 120.84196 to 118.56646, saving model to best_model.h5\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 244.6223 - val_loss: 118.5665\n",
            "Epoch 10/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 250.2498\n",
            "Epoch 10: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 21ms/step - loss: 250.2498 - val_loss: 138.9940\n",
            "Epoch 11/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 245.0129\n",
            "Epoch 11: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 20ms/step - loss: 244.4983 - val_loss: 123.7253\n",
            "Epoch 12/32\n",
            "58/60 [============================>.] - ETA: 0s - loss: 240.1523\n",
            "Epoch 12: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 21ms/step - loss: 239.9999 - val_loss: 127.8843\n",
            "Epoch 13/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 236.0973\n",
            "Epoch 13: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 236.0973 - val_loss: 126.3998\n",
            "Epoch 14/32\n",
            "58/60 [============================>.] - ETA: 0s - loss: 225.8417\n",
            "Epoch 14: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 21ms/step - loss: 227.2923 - val_loss: 136.7326\n",
            "Epoch 15/32\n",
            "58/60 [============================>.] - ETA: 0s - loss: 226.6456\n",
            "Epoch 15: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 20ms/step - loss: 229.1558 - val_loss: 134.7854\n",
            "Epoch 16/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 225.2846\n",
            "Epoch 16: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 34ms/step - loss: 225.2846 - val_loss: 151.2188\n",
            "Epoch 17/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 225.3908\n",
            "Epoch 17: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 40ms/step - loss: 225.3908 - val_loss: 152.6901\n",
            "Epoch 18/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 222.2057\n",
            "Epoch 18: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 221.7429 - val_loss: 139.8926\n",
            "Epoch 19/32\n",
            "58/60 [============================>.] - ETA: 0s - loss: 212.1264\n",
            "Epoch 19: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 19ms/step - loss: 215.8797 - val_loss: 147.0743\n",
            "Epoch 20/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 217.3203\n",
            "Epoch 20: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 20ms/step - loss: 216.8826 - val_loss: 152.4496\n",
            "Epoch 21/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 216.1725\n",
            "Epoch 21: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 215.7656 - val_loss: 138.2250\n",
            "Epoch 22/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 221.4445\n",
            "Epoch 22: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 22ms/step - loss: 221.4445 - val_loss: 140.6949\n",
            "Epoch 23/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 215.5794\n",
            "Epoch 23: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 30ms/step - loss: 215.5794 - val_loss: 148.1526\n",
            "Epoch 24/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 209.1119\n",
            "Epoch 24: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 20ms/step - loss: 208.6813 - val_loss: 151.7137\n",
            "Epoch 25/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 208.9400\n",
            "Epoch 25: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 31ms/step - loss: 208.5012 - val_loss: 169.6940\n",
            "Epoch 26/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 213.3102\n",
            "Epoch 26: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 38ms/step - loss: 212.9875 - val_loss: 155.0117\n",
            "Epoch 27/32\n",
            "58/60 [============================>.] - ETA: 0s - loss: 205.2894\n",
            "Epoch 27: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 206.7996 - val_loss: 150.2036\n",
            "Epoch 28/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 203.2322\n",
            "Epoch 28: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 25ms/step - loss: 202.8065 - val_loss: 172.6126\n",
            "Epoch 29/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 204.5132\n",
            "Epoch 29: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 33ms/step - loss: 204.5132 - val_loss: 166.5218\n",
            "Epoch 30/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 202.2518\n",
            "Epoch 30: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 2s 28ms/step - loss: 202.2518 - val_loss: 166.7578\n",
            "Epoch 31/32\n",
            "59/60 [============================>.] - ETA: 0s - loss: 197.6683\n",
            "Epoch 31: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 25ms/step - loss: 197.3900 - val_loss: 158.9676\n",
            "Epoch 32/32\n",
            "60/60 [==============================] - ETA: 0s - loss: 193.6724\n",
            "Epoch 32: val_loss did not improve from 118.56646\n",
            "60/60 [==============================] - 1s 25ms/step - loss: 193.6724 - val_loss: 175.2546\n",
            "2/2 [==============================] - 2s 15ms/step\n",
            "Mean Squared Error: 118.557144\n",
            "mae: 14.079099\n",
            "rse: 0.5997365\n",
            "rmse: 10.888395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(10):\n",
        "  print(f\"input data{X_test[i]} \\n: Output:{y_test[i]}     {predictions[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BH1NJZb-h_X",
        "outputId": "8da8f32b-7979-4be3-9adf-b1698aa7f5c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input data[[ 30.7575  12.0575   0.       0.6      1.     151.    ]\n",
            " [ 31.0075  10.0075   0.       0.       1.     142.    ]] \n",
            ": Output:0.0     [1.6724901]\n",
            "input data[[20.0075   17.1075    0.       21.        0.        0.      ]\n",
            " [19.4075   17.2575    0.       27.300001  0.        0.      ]] \n",
            ": Output:9.300000190734863     [26.405493]\n",
            "input data[[21.8075 18.0075  0.      0.      1.     48.    ]\n",
            " [24.4575 18.4075  0.      0.      1.     95.    ]] \n",
            ": Output:0.0     [1.142267]\n",
            "input data[[21.7575 14.0575  0.      0.      0.      0.    ]\n",
            " [21.0575 13.5575  0.      0.6     0.      0.    ]] \n",
            ": Output:0.0     [2.0475233]\n",
            "input data[[ 28.5575    15.9075     0.        27.300001   1.       177.      ]\n",
            " [ 28.0075    13.4075     0.        38.4        1.       169.      ]] \n",
            ": Output:15.600000381469727     [31.62913]\n",
            "input data[[21.4075 13.4075  0.     23.1     1.     48.    ]\n",
            " [23.1575 11.5575  0.      2.4     0.      2.    ]] \n",
            ": Output:44.70000076293945     [15.112878]\n",
            "input data[[18.5575 18.4075  0.      0.      0.      0.    ]\n",
            " [19.3575 19.2575  0.     94.8     1.      4.    ]] \n",
            ": Output:52.19999694824219     [34.21326]\n",
            "input data[[24.0075   15.9075    0.        0.6       0.        0.      ]\n",
            " [22.8075   16.307499  0.        0.        0.        0.      ]] \n",
            ": Output:0.0     [2.4551928]\n",
            "input data[[ 24.7575  17.6075   0.       2.4      1.     132.    ]\n",
            " [ 26.2075  17.0575   0.       1.5      1.     151.    ]] \n",
            ": Output:5.400000095367432     [2.4161282]\n",
            "input data[[2.7907499e+01 1.7707500e+01 1.0000000e-01 6.6000004e+00 1.0000000e+00\n",
            "  1.6200000e+02]\n",
            " [2.8057501e+01 1.7457500e+01 0.0000000e+00 1.6799999e+01 1.0000000e+00\n",
            "  1.5200000e+02]] \n",
            ": Output:5.400000095367432     [25.123417]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRIAL"
      ],
      "metadata": {
        "id": "myvrpTbfhc0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Bidirectional,Dropout\n",
        "from keras.optimizers import Adam\n",
        "data=hourly_dataframe.copy()\n",
        "data.drop(columns=['date'], inplace=True)\n",
        "# Calculate correlation between features and target variable\n",
        "correlation = data.corr()['cloud_cover'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Define the threshold for correlation\n",
        "correlation_threshold = 0.1 # Adjust as needed\n",
        "\n",
        "# Split features into two groups based on correlation\n",
        "high_corr_features = correlation[correlation >= correlation_threshold].index.tolist()\n",
        "low_corr_features = correlation[correlation < correlation_threshold].index.tolist()\n",
        "\n",
        "\n",
        "# Define preprocessing steps for high correlation features using IterativeImputer\n",
        "high_corr_transformer = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer(max_iter=10, random_state=42)),  # IterativeImputer for high correlation features\n",
        "    # Add more preprocessing steps if necessary\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('high_corr', high_corr_transformer, high_corr_features),\n",
        "        ('low_corr', 'drop', low_corr_features)  # Drop low correlated features\n",
        "    ],\n",
        "    remainder='passthrough'  # Include any remaining columns not specified in transformers\n",
        ")\n",
        "# Apply preprocessing steps to the data\n",
        "data_preprocessed = preprocessor.fit_transform(data)\n",
        "# Get the feature names of the transformed data\n",
        "feature_names_out = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Print the feature names\n",
        "print(feature_names_out)\n",
        "\n",
        "\n",
        "# Rearrange data into time series format\n",
        "time_steps = 2 # Define the number of time steps\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(data_preprocessed) - time_steps):\n",
        "    X.append(data_preprocessed[i:i+time_steps])\n",
        "    y.append(data_preprocessed[i+time_steps][0])\n",
        "print(y)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split the data into training and testing sets (80%-20% and 90%-10%)\n",
        "X_train_80, X_test_20, y_train_80, y_test_20 = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/best_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "input_shape = input_details[0]['shape']\n",
        "print(\"Expected input shape:\", input_shape)\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Prepare input data (similar to what was done during training)\n",
        "# Assuming X_test_20 is your test data\n",
        "# Prepare input data (similar to what was done during training)\n",
        "input_data = np.array(X_test_20, dtype=np.float32)\n",
        "print(\"Input data shape:\", input_data.shape)\n",
        "\n",
        "# Prepare an array to store predictions for each sample\n",
        "predictions_tflite = []\n",
        "\n",
        "# Loop through each sample in the input data\n",
        "for sample in input_data:\n",
        "    # Reshape each sample to match the expected input shape of the model\n",
        "    sample = np.reshape(sample, (1,2,6))  # Exclude the batch dimension\n",
        "\n",
        "    # Run inference for the current sample\n",
        "    interpreter.set_tensor(input_details[0]['index'], sample)\n",
        "    interpreter.invoke()\n",
        "    prediction = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # Collect predictions\n",
        "    predictions_tflite.append(prediction[0]/2.87)\n",
        "\n",
        "# Convert predictions to a numpy array\n",
        "predictions_tflite = np.array(predictions_tflite)\n",
        "\n",
        "# Calculate Mean Squared Error for all predictions\n",
        "for i in range(20):\n",
        "  print(f\"{X_test_20[i]} :{y_test_20[i]} : {predictions_tflite[i]}\")\n",
        "mse_tflite = mean_squared_error(y_test_20, predictions_tflite)\n",
        "\n",
        "print(\"Mean Squared Error (TFLite) for all samples:\", mse_tflite)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn6BBr3whe9v",
        "outputId": "7774654c-8f99-4019-c5fd-1a59780db513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['high_corr__cloud_cover' 'high_corr__precipitation'\n",
            " 'high_corr__diffuse_radiation' 'high_corr__dew_point_2m'\n",
            " 'high_corr__is_day' 'high_corr__temperature_2m']\n",
            "[18.600002, 16.5, 12.900001, 15.0, 18.0, 67.5, 60.300003, 82.200005, 19.5, 46.5, 59.100002, 13.500001, 15.6, 20.400002, 32.4, 13.200001, 13.200001, 7.8, 1.8000001, 0.6, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.5, 11.4, 6.6000004, 16.8, 5.4, 4.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8000001, 4.2000003, 3.6, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.6, 4.8, 6.6000004, 7.2000003, 0.90000004, 0.0, 76.5, 65.7, 68.1, 31.8, 6.6000004, 9.6, 4.8, 11.400001, 21.0, 9.6, 4.8, 3.6000001, 0.6, 2.4, 9.3, 4.2000003, 17.400002, 22.5, 22.500002, 28.500002, 24.300001, 21.300001, 3.9, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 9.6, 32.4, 12.0, 12.0, 26.400002, 21.0, 50.4, 6.3, 40.5, 23.400002, 29.400002, 4.8, 0.6, 0.0, 0.0, 0.0, 0.0, 0.6, 1.2, 6.3, 3.6000001, 2.7, 15.900001, 67.200005, 25.800001, 34.800003, 41.4, 43.800003, 33.0, 42.0, 27.300001, 34.2, 27.600002, 26.7, 19.2, 19.5, 29.7, 26.700003, 13.5, 21.0, 27.300001, 9.3, 12.900001, 24.6, 34.800003, 31.2, 30.900002, 20.400002, 28.2, 34.5, 41.700005, 22.5, 29.7, 26.7, 7.8, 14.1, 42.000004, 29.400002, 16.2, 0.6, 9.0, 56.7, 72.9, 19.5, 12.0, 17.400002, 9.6, 3.0, 0.0, 9.0, 2.4, 1.5, 5.4, 17.7, 27.300001, 38.4, 15.6, 9.0, 1.8000001, 3.0, 60.000004, 39.0, 9.0, 21.6, 0.0, 0.0, 0.0, 3.2999997, 0.0, 5.1, 6.3, 12.900001, 30.300001, 30.000002, 59.4, 68.100006, 64.5, 63.600002, 55.800003, 27.000002, 26.7, 5.1000004, 23.1, 2.4, 44.7, 71.700005, 70.8, 82.8, 2.4, 15.0, 18.300001, 28.2, 18.0, 58.5, 92.399994, 34.5, 18.0, 13.8, 66.3, 61.800003, 22.8, 11.4, 3.8999999, 19.800001, 51.6, 38.4, 46.800003, 58.2, 15.0, 8.400001, 4.8, 2.6999998, 0.6, 5.4, 2.7, 4.5, 5.3999996, 14.1, 11.7, 14.7, 17.7, 7.5, 24.900002, 0.3, 0.9, 16.5, 16.5, 12.0, 38.4, 39.9, 38.100002, 6.9000006, 1.2, 14.4, 12.9, 42.6, 29.7, 31.800001, 30.000002, 30.000002, 30.000002, 30.000002, 27.900002, 30.000002, 30.000002, 30.000002, 15.000001, 29.7, 30.000002, 30.000002, 30.000002, 28.800001, 8.1, 12.900001, 41.4, 66.0, 90.00001, 90.00001, 79.8, 38.100002, 17.7, 0.0, 27.0, 23.4, 0.3, 9.0, 0.0, 94.8, 52.199997, 6.2999997, 39.6, 20.699999, 24.6, 17.1, 36.0, 31.5, 17.4, 6.6000004, 1.8000001, 16.800001, 1.8000001, 0.0, 2.4, 3.6000001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.7999997, 14.4, 0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.2000003, 7.8, 16.800001, 60.000004, 52.2, 21.6, 13.200001, 2.4, 1.8000001, 0.6, 0.6, 0.0, 0.0, 0.6, 1.2, 27.6, 51.900005, 25.800001, 27.6, 30.900002, 63.9, 62.400005, 60.300003, 48.3, 14.1, 20.400002, 30.900002, 4.2000003, 3.6000001, 25.2, 0.0, 0.6, 8.400001, 19.800001, 64.2, 41.1, 0.6, 0.6, 1.2, 7.8, 2.7, 3.0, 5.4, 9.3, 18.3, 63.300003, 45.3, 68.1, 45.6, 17.1, 22.2, 3.0, 35.4, 7.8, 0.0, 1.8000001, 0.0, 0.9, 11.7, 10.5, 4.5, 28.8, 42.3, 90.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.3, 6.6000004, 9.0, 12.0, 1.8000001, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 19.5, 4.2000003, 6.0, 6.0, 1.2, 0.6, 0.0, 0.0, 0.0, 0.0, 3.6000001, 2.4, 3.0, 10.8, 17.400002, 41.4, 53.7, 36.0, 60.000004, 60.000004, 8.400001, 0.0, 9.0, 7.2000003, 8.400001, 19.8, 18.6, 5.4, 24.0, 14.400001, 0.6, 0.6, 38.4, 22.8]\n",
            "Expected input shape: [1 2 6]\n",
            "Input data shape: (106, 2, 6)\n",
            "[[17.7     0.      0.     19.8575  0.     20.2575]\n",
            " [ 0.      0.      0.     19.1575  0.     19.2575]] :27.0 : [12.586188]\n",
            "[[  0.       0.     150.      12.7075   1.      32.0575]\n",
            " [  0.       0.     140.      11.9575   1.      32.2575]] :4.200000286102295 : [9.235811]\n",
            "[[18.      0.1     0.     17.5075  1.     18.9575]\n",
            " [67.5     0.     42.     17.9075  1.     19.4575]] :60.30000305175781 : [6.7989836]\n",
            "[[26.7     0.     98.     12.5575  1.     30.6075]\n",
            " [ 7.8     0.     49.     12.1075  1.     28.9075]] :14.100000381469727 : [10.367077]\n",
            "[[ 0.      0.      1.     12.2075  1.     14.7575]\n",
            " [ 0.      0.     48.     13.4575  1.     17.5075]] :0.0 : [11.359888]\n",
            "[[ 30.000002   0.       260.        17.0575     1.        29.3575  ]\n",
            " [ 28.800001   0.       284.        17.1075     1.        29.4575  ]] :8.100000381469727 : [7.3434095]\n",
            "[[ 0.6     0.      0.     18.8075  0.     21.4575]\n",
            " [38.4     0.      0.     18.3075  0.     20.9575]] :22.799999237060547 : [9.834016]\n",
            "[[  0.       0.     108.      15.8075   1.      23.2575]\n",
            " [  0.       0.     133.      16.2075   1.      25.2575]] :0.0 : [9.326855]\n",
            "[[ 4.2000003  0.         0.        17.0575     0.        21.4575   ]\n",
            " [17.400002   0.         0.        16.9075     0.        22.6575   ]] :22.5 : [11.840455]\n",
            "[[50.4     0.     42.     15.1575  1.     28.2575]\n",
            " [ 6.3     0.      1.     15.3575  0.     25.8075]] :40.5 : [11.909179]\n",
            "[[ 19.5      0.5    133.      18.1075   1.      20.9575]\n",
            " [ 46.5      0.     226.      18.9575   1.      23.4075]] :59.10000228881836 : [7.5832763]\n",
            "[[ 60.000004   0.       160.        17.2075     1.        22.2575  ]\n",
            " [ 60.000004   0.       142.        17.7075     1.        23.2075  ]] :8.40000057220459 : [5.519613]\n",
            "[[ 0.      0.      0.      8.7575  0.     14.5075]\n",
            " [ 0.      0.      0.      9.3575  0.     13.5575]] :0.0 : [11.845806]\n",
            "[[  3.       0.     111.      20.8575   1.      24.1075]\n",
            " [  5.4      0.     146.      20.5575   1.      26.3575]] :9.300000190734863 : [9.327323]\n",
            "[[ 0.      0.      0.     14.1575  0.     18.7075]\n",
            " [ 0.      0.      0.     14.1575  0.     17.8075]] :0.0 : [12.355203]\n",
            "[[23.400002  0.        0.       15.1075    0.       24.8075  ]\n",
            " [29.400002  0.        0.       14.7575    0.       24.1075  ]] :4.800000190734863 : [10.078272]\n",
            "[[18.600002  0.        0.       16.7075    0.       18.0575  ]\n",
            " [16.5       0.        0.       17.1075    0.       19.0575  ]] :12.90000057220459 : [11.908282]\n",
            "[[ 25.800001   0.       263.        15.6575     1.        29.7075  ]\n",
            " [ 34.800003   0.       217.        14.5075     1.        31.2575  ]] :41.400001525878906 : [7.5250607]\n",
            "[[ 0.      0.     53.     16.0075  1.     26.3075]\n",
            " [ 0.      0.      3.     11.9075  0.     27.0075]] :0.0 : [11.567839]\n",
            "[[9.60000e+00 0.00000e+00 1.44000e+02 1.82075e+01 1.00000e+00 3.10575e+01]\n",
            " [3.24000e+01 1.00000e-01 1.57000e+02 1.43075e+01 1.00000e+00 3.19075e+01]] :12.0 : [8.757363]\n",
            "Mean Squared Error (TFLite) for all samples: 419.89014\n"
          ]
        }
      ]
    }
  ]
}